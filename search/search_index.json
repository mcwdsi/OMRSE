{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OMRSE Ontology Documentation Welcome to the OMRSE documentation! You can find descriptions of the standard ontology engineering workflows here .","title":"Overview"},{"location":"#omrse-ontology-documentation","text":"Welcome to the OMRSE documentation! You can find descriptions of the standard ontology engineering workflows here .","title":"OMRSE Ontology Documentation"},{"location":"cite/","text":"How to cite OMRSE","title":"Cite"},{"location":"cite/#how-to-cite-omrse","text":"","title":"How to cite OMRSE"},{"location":"contributing/","text":"How to contribute to OMRSE","title":"Contributing"},{"location":"contributing/#how-to-contribute-to-omrse","text":"","title":"How to contribute to OMRSE"},{"location":"history/","text":"A brief history of OMRSE The following page gives an overview of the history of OMRSE.","title":"History"},{"location":"history/#a-brief-history-of-omrse","text":"The following page gives an overview of the history of OMRSE.","title":"A brief history of OMRSE"},{"location":"modeling/Home/","text":"Welcome to the OMRSE wiki! Read an overview of the OMRSE ontology Specific modeling decisions: Healthcare facilities Hospital discharge & discharge status Householder Housing unit & household Insurance, especially health insurance Language Language as codified by ISO-639 Race and ethnicity Relation between a role and the organization that creates it Smoking status","title":"Modeling home"},{"location":"modeling/Home/#welcome-to-the-omrse-wiki","text":"Read an overview of the OMRSE ontology","title":"Welcome to the OMRSE wiki!"},{"location":"modeling/Home/#specific-modeling-decisions","text":"Healthcare facilities Hospital discharge & discharge status Householder Housing unit & household Insurance, especially health insurance Language Language as codified by ISO-639 Race and ethnicity Relation between a role and the organization that creates it Smoking status","title":"Specific modeling decisions:"},{"location":"modeling/Householder/","text":"US Census Householder US Census \"The householder refers to the person (or one of the people) in whose name the housing unit is owned or rented (maintained) or, if there is no such person, any adult member, excluding roomers, boarders, or paid employees. If the house is owned or rented jointly by a married couple, the householder may be either the husband or the wife. The person designated as the householder is the \"reference person\" to whom the relationship of all other household members, if any, is recorded. \"The number of householders is equal to the number of households. Also, the number of family householders is equal to the number of families. \"Head versus householder. Beginning with the 1980 CPS, the Bureau of the Census discontinued the use of the terms \"head of household\" and \"head of family.\" Instead, the terms \"householder\" and \"family householder\" are used. Recent social changes have resulted in greater sharing of household responsibilities among the adult members and, therefore, have made the term \"head\" increasingly inappropriate in the analysis of household and family data. Specifically, beginning in 1980, the Census Bureau discontinued its longtime practice of always classifying the husband as the reference person (head) when he and his wife are living together.\" http://www.census.gov/cps/about/cpsdef.html Sufficient condition - being the only person who maintains (owns or rents) the housing unit in their name. Necessary conditions: adult, member of the household, not paying another member of the household to live there; not a paid employee of the household. Notes: There is one householder to household. The householder role is individuated by households. Can one person by the householder of many households? (Look at Utah census data.) The householder also plays the role of US Census reference person. Proposed definition: A US Census householder role is a human social role that inheres in a Homo sapiens and is realized by that person being a member of a household and either owning or renting the housing unit in which that household resides and being designated as the householder. If there is only one member of the household who owns or rents the housing unit, that person is designated the householder by default. US Census Reference Person \"The reference person is the person to whom the relationship of other people in the household is recorded. The household reference person is the person listed as the householder (see definition of \"Householder\"). The subfamily reference person is either the single parent or the husband/wife in a married-couple situation.\" Do we need to model \"US Census reference person\"?","title":"US Census Householder"},{"location":"modeling/Householder/#us-census-householder","text":"US Census \"The householder refers to the person (or one of the people) in whose name the housing unit is owned or rented (maintained) or, if there is no such person, any adult member, excluding roomers, boarders, or paid employees. If the house is owned or rented jointly by a married couple, the householder may be either the husband or the wife. The person designated as the householder is the \"reference person\" to whom the relationship of all other household members, if any, is recorded. \"The number of householders is equal to the number of households. Also, the number of family householders is equal to the number of families. \"Head versus householder. Beginning with the 1980 CPS, the Bureau of the Census discontinued the use of the terms \"head of household\" and \"head of family.\" Instead, the terms \"householder\" and \"family householder\" are used. Recent social changes have resulted in greater sharing of household responsibilities among the adult members and, therefore, have made the term \"head\" increasingly inappropriate in the analysis of household and family data. Specifically, beginning in 1980, the Census Bureau discontinued its longtime practice of always classifying the husband as the reference person (head) when he and his wife are living together.\" http://www.census.gov/cps/about/cpsdef.html Sufficient condition - being the only person who maintains (owns or rents) the housing unit in their name. Necessary conditions: adult, member of the household, not paying another member of the household to live there; not a paid employee of the household. Notes: There is one householder to household. The householder role is individuated by households. Can one person by the householder of many households? (Look at Utah census data.) The householder also plays the role of US Census reference person. Proposed definition: A US Census householder role is a human social role that inheres in a Homo sapiens and is realized by that person being a member of a household and either owning or renting the housing unit in which that household resides and being designated as the householder. If there is only one member of the household who owns or rents the housing unit, that person is designated the householder by default.","title":"US Census Householder"},{"location":"modeling/Householder/#us-census-reference-person","text":"\"The reference person is the person to whom the relationship of other people in the household is recorded. The household reference person is the person listed as the householder (see definition of \"Householder\"). The subfamily reference person is either the single parent or the husband/wife in a married-couple situation.\" Do we need to model \"US Census reference person\"?","title":"US Census Reference Person"},{"location":"modeling/Housing-unit-and-Household/","text":"Use Cases Representing US Census Housing Units for Exposome Research We currently have a need to integrate American Community Survey (ACS) data for Florida with other environmental datasets in a graph database to support exposome research. These data capture measures of aggregates of households within a particular region in the US (e.g., Census tracts, counties, etc.). Representing Synthetic Ecosystem for the Modeling Infectious Disease Agent Study We previously had a use case to represent households and housing units in the Modeling Infectious Disease Agent Study (MIDAS) Informatics Services Group (ISG). Specifically, we represented synthetic ecosystem datasets, which are datasets that are derived from so-called \"micro-samples\" of actual census data. To build agent-based epidemic simulators, researchers will often take samples of census data and expand them back up to the size of the entire population according to statistical algorithms that ensure the re-created overall population dataset matches the actual population in terms of race, gender, ethnicity, geographical distribution, and so on. For modelers, finding a synthetic ecosystem that meets their needs in terms of geography, other entities represented (such as households, schools, and workplaces) is a key issue. Therefore, we built the Ontology-based Catalog for Infectious Disease Epidemiology (OBC.ide) to help modelers and analysts find synthetic ecosystems and other information resources. We wish to transform some existing synthetic ecosystem data into RDF and need key ontology classes for it. Two of these classes are housing unit and household. Background Information Because the data for our use cases are based on US Census data, we propose initially to use definitions from the US Census: household : The Census defines it as ... all the people who occupy a housing unit. Source: http://www.census.gov/cps/about/cpsdef.html. housing unit : The Census says A house, an apartment or other group of rooms, or a single room, is regarded as a housing unit when it is occupied or intended for occupancy as separate living quarters; that is, when the occupants do not live with any other persons in the structure and there is direct access from the outside or through a common hall. Per OMRSE, the entire apartment building would be an instance of architectural structure . So each apartment unit in it is not an architectural structure. It is a part of it. However, in the case of a detached, single-family home, the housing unit is indeed an architectural structure. So at minimum, a housing unit is a part of an architectural structure (the reflexivity of part of handles the single-family home case). The Census criterion of direct access from outside or through a hall common to other housing units is a good one. Also we note that mobile homes are sufficiently attached to the ground when serving as a housing unit to meet the definition of 'architectural structure'. One key issue to decide is what about house boats and recreational vehicles? They clearly are not architectural structures (lack connection to ground). But they serve a particular housing function. Here is how the US Census addresses the issue: Both occupied and vacant housing units are included in the housing unit inventory, except that recreational vehicles, boats, vans, tents, railroad cars, and the like are included only if they are occupied as someone's usual place of residence. Vacant mobile homes are included provided they are intended for occupancy on the site where they stand. Vacant mobile homes on dealer's sales lots, at the factory, or in storage yards are excluded from the housing unit inventory. Source: https://www.census.gov/popest/about/terms/housing.html. So a house boat or a recreational vehicle is only a housing unit if it is someone's \"usual place of residence\", else it is not. A mobile home is only a housing unit if it is an architectural structure. Definitions housing unit : A material entity that has as parts one or more sites large enough to contain humans, has as part one or more material entities that separates it from other sites, and bears a residence function. household : A human or collection of humans that occupies a housing unit by storing their possessions there and habitually sleeping there thereby participating in the realization of that housing unit's residence function. This definition is intended to cover vehicular residences, architectural residences, natural geological formation residences, and other types of material entities that can serve as housing for someone. residence function : A function that inheres in a material entity and, if realized, is realized by protecting persons and their possessions from weather and by some person or group of persons habitually sleeping in at least one site that is contained by that material entity. For the U.S. Census, architectural structures that bear a residence function and vehicles that are realizing a residence function, are housing units. Also, the U.S. Census says a mobile home does not bear a residence function until it becomes an architectural structure (it is suitably sited and anchored to the ground). Similarly, tents are not housing units unless they are realizing a residence function. A tent does not have as \"firm [a] connection between its foundation and the ground\" so we would exclude them as architectural structures. They don't fit category of vehicle or natural formation either, but may still be important to include to cover folks who are unhoused and live on the streets. To make it easier to relate a person or household to the housing unit they live in, we created the object property lives in (OMRSE:00000260). This object property relates a household to another material entity. lives in : A relation between a household and a material entity that the household stores their possessions in and sleeps in habitually.","title":"Housing unit and Household"},{"location":"modeling/Housing-unit-and-Household/#use-cases","text":"","title":"Use Cases"},{"location":"modeling/Housing-unit-and-Household/#representing-us-census-housing-units-for-exposome-research","text":"We currently have a need to integrate American Community Survey (ACS) data for Florida with other environmental datasets in a graph database to support exposome research. These data capture measures of aggregates of households within a particular region in the US (e.g., Census tracts, counties, etc.).","title":"Representing US Census Housing Units for Exposome Research"},{"location":"modeling/Housing-unit-and-Household/#representing-synthetic-ecosystem-for-the-modeling-infectious-disease-agent-study","text":"We previously had a use case to represent households and housing units in the Modeling Infectious Disease Agent Study (MIDAS) Informatics Services Group (ISG). Specifically, we represented synthetic ecosystem datasets, which are datasets that are derived from so-called \"micro-samples\" of actual census data. To build agent-based epidemic simulators, researchers will often take samples of census data and expand them back up to the size of the entire population according to statistical algorithms that ensure the re-created overall population dataset matches the actual population in terms of race, gender, ethnicity, geographical distribution, and so on. For modelers, finding a synthetic ecosystem that meets their needs in terms of geography, other entities represented (such as households, schools, and workplaces) is a key issue. Therefore, we built the Ontology-based Catalog for Infectious Disease Epidemiology (OBC.ide) to help modelers and analysts find synthetic ecosystems and other information resources. We wish to transform some existing synthetic ecosystem data into RDF and need key ontology classes for it. Two of these classes are housing unit and household.","title":"Representing Synthetic Ecosystem for the Modeling Infectious Disease Agent Study"},{"location":"modeling/Housing-unit-and-Household/#background-information","text":"Because the data for our use cases are based on US Census data, we propose initially to use definitions from the US Census: household : The Census defines it as ... all the people who occupy a housing unit. Source: http://www.census.gov/cps/about/cpsdef.html. housing unit : The Census says A house, an apartment or other group of rooms, or a single room, is regarded as a housing unit when it is occupied or intended for occupancy as separate living quarters; that is, when the occupants do not live with any other persons in the structure and there is direct access from the outside or through a common hall. Per OMRSE, the entire apartment building would be an instance of architectural structure . So each apartment unit in it is not an architectural structure. It is a part of it. However, in the case of a detached, single-family home, the housing unit is indeed an architectural structure. So at minimum, a housing unit is a part of an architectural structure (the reflexivity of part of handles the single-family home case). The Census criterion of direct access from outside or through a hall common to other housing units is a good one. Also we note that mobile homes are sufficiently attached to the ground when serving as a housing unit to meet the definition of 'architectural structure'. One key issue to decide is what about house boats and recreational vehicles? They clearly are not architectural structures (lack connection to ground). But they serve a particular housing function. Here is how the US Census addresses the issue: Both occupied and vacant housing units are included in the housing unit inventory, except that recreational vehicles, boats, vans, tents, railroad cars, and the like are included only if they are occupied as someone's usual place of residence. Vacant mobile homes are included provided they are intended for occupancy on the site where they stand. Vacant mobile homes on dealer's sales lots, at the factory, or in storage yards are excluded from the housing unit inventory. Source: https://www.census.gov/popest/about/terms/housing.html. So a house boat or a recreational vehicle is only a housing unit if it is someone's \"usual place of residence\", else it is not. A mobile home is only a housing unit if it is an architectural structure.","title":"Background Information"},{"location":"modeling/Housing-unit-and-Household/#definitions","text":"housing unit : A material entity that has as parts one or more sites large enough to contain humans, has as part one or more material entities that separates it from other sites, and bears a residence function. household : A human or collection of humans that occupies a housing unit by storing their possessions there and habitually sleeping there thereby participating in the realization of that housing unit's residence function. This definition is intended to cover vehicular residences, architectural residences, natural geological formation residences, and other types of material entities that can serve as housing for someone. residence function : A function that inheres in a material entity and, if realized, is realized by protecting persons and their possessions from weather and by some person or group of persons habitually sleeping in at least one site that is contained by that material entity. For the U.S. Census, architectural structures that bear a residence function and vehicles that are realizing a residence function, are housing units. Also, the U.S. Census says a mobile home does not bear a residence function until it becomes an architectural structure (it is suitably sited and anchored to the ground). Similarly, tents are not housing units unless they are realizing a residence function. A tent does not have as \"firm [a] connection between its foundation and the ground\" so we would exclude them as architectural structures. They don't fit category of vehicle or natural formation either, but may still be important to include to cover folks who are unhoused and live on the streets. To make it easier to relate a person or household to the housing unit they live in, we created the object property lives in (OMRSE:00000260). This object property relates a household to another material entity. lives in : A relation between a household and a material entity that the household stores their possessions in and sleeps in habitually.","title":"Definitions"},{"location":"modeling/How-are-household-and-housing-unit-related%3F/","text":"The members of the household are participants of a process that realizes the residence function. Whether they or the housing unit itself is the agent of the process is an interesting question. So if we had particulars as follows: hh1 - household #1 hu1 - housing unit #1 rf1 - residence function #1 p1 - process #1 hh1 'is participant of' p1 p1 'realizes' rf1 rf1 'inheres in' hu1 One issue is that it means that there is no connection between a household and the housing unit until at least one member of the household sleeps there. Based on my (Bill Hogan) own experience with moving to Gainesville last year, I'm down with it.","title":"How are household and housing unit related?"},{"location":"modeling/Insurance/","text":"This page discusses questions relevant to modeling insurance and insurance policies in OMRSE In Clarke v Clarke, (1993) 84 BCLR 2d 98, the BC Supreme Court accepted this definition (of life or disability insurance): \"A contract by which one party undertakes, in consideration for a payment (called a premium), to secure the other against pecuniary loss, by payment of a sum of money in the event of the death or disablement of a person.\"(http://www.duhaime.org/LegalDictionary/I/Insurance.aspx, accessed July 20, 2015) And \"A contract whereby, for specified consideration, one party undertakes to compensate the other for a loss relating to a particular subject as a result of the occurrence of designated hazards. \u2026 A contract is considered to be insurance if it distributes risk among a large number of persons through an enterprise that is engaged primarily in the business of insurance. Warranties or service contracts for merchandise, for example, do not constitute insurance. They are not issued by insurance companies, and the risk distribution in the transaction is incidental to the purchase of the merchandise. Warranties and service contracts are thus exempt from strict insurance laws and regulations.\u201d (http://legal-dictionary.thefreedictionary.com/insurance, accessed July 20, 2015) \"A contract (an insurance contract) whereby one person, the insurer, promises and undertakes, in exchange for consideration of a set or assessed amount of money (called a \"premium\"), to make a payment to either the insured or a third-party if a specified event occurs, also known as \"occurrences\".\" (http://www.duhaime.org/LegalDictionary/I/Insurance.aspx, accessed July 20, 2015) From Couch on Insurance, 3rd Edition: \"while a policy of insurance, other than life or accident insurance, is basically a contract of indemnity, not all contracts of indemnity are insurance contracts; rather, an insurance contract is one type of indemnity contract.\u201d An insurance policy is a contract . More specifically, it's a type of indemnity contract . What's the relationship between a contract and a document act? An insurance policy is a contract that is a document that has as parts action, conditional, plan, and objective specifications. It is the specified output of a document act. It is distinguished from other indemnity contracts by distributing the risk among a group of persons through an organization. Insurance policies are the specified output of a document act. (Is this right?) That document act has as participants (1) a group of persons (the insured parties ) and (2) the organization that issues the plan. (From 2) The organization and the primary insured persons on the policy are parties to a legal agreement (an insurance policy). An insurance company is an organization and bearer_of some payor_role that is realized by making a payment to the insured or a third party once (in the case of health insurance) health services are provided to the insured. The payor role (in this case, not generally) is the concretization of a socio-legal generically dependent_continuant that is the specified output of some document act and inheres in an organization that is party to a insurance policy. The payor role is the subject of the action specification that is a part of the insurance policy as is the payment. The enrollment date is the day that the payor and insured roles came into existence. Or perhaps the SLGDCs that the roles concretize. (Note that the insured role is not generically dependent since one cannot transfers one's insurance benefits to another person.) An insured party role is the subject of a conditional specification that is a part of some insurance policy and is the specified output of the document act that also has the insurance policy as specified output. An enrollment start date is a date that contains the left boundary of the existence of the insured party role. A date is a temporal interval that has a scalar measurement datum whose value is equivalent to one day. An enrollment in an insurance policy period is a temporal interval during which an organism is the bearer of an insured party role. The enrollment in an insurance policy period is also a part of the temporal interval occupied by the life of that organism. New Terms contract - Superclass: Document and (is_specified_output_of some 'document act') indemnity contract - Superclass: contract and ('has part' some 'action specification') and ('has part' some 'objective specification') and ('has part' some 'plan specification') and ('has part' some 'conditional specification') insurance policy - Superclass: 'indemnity contract' and is_specified_output_of some ('document act' and (has_agent some 'insurance company') and (has_agent some ('collection of humans' and 'has member' only (bearer_of some 'policy holder role')))) ['and has_specified_output some 'insured party role'' should modify 'document act', but this is not possible while declaration is defined as having a SLGDC as specified output since roles are not generically dependent.] insured party role - Superclass: 'role in human social processes' and (inverse 'is about' some ('conditional specification' and 'part of' some 'insurance policy')) and (is_specified_output_of some ('document act' and (has_specified_output some 'insurance policy'))) insurance company - Superclass: organization and ('bearer of' some ('payer role' and (concretizes some ('socio-legal generically dependent continuant' and is_specified_output_of some 'document act')))) and ('bearer of' some 'party to an insurance policy') and (inverse 'is about' some ('action specification' and 'part of' some 'insurance policy')) policy holder role - Superclass: 'insured party role' and ('inheres in' some ('bearer of' some 'party to an insurance policy')) payer role - Superclass: 'role in human social processes' party to an insurance policy - Superclass: 'party to a legal agreement' enrollment in an insurance policy period - Subclass: 'temporal interval' and (inverse(exists at) ('insured part role' and 'inheres in' (some organism))) and ('part of' some ('temporal interval' and 'is temporal location of' [life of an organism]) enrollment start date - date and 'is occupied by' some ('history part' and 'has left process boundary' some ('process boundary' and ('part of' some 'enrollment in an insurance policy period')) enrollment end date - date and 'is occupied by' some ('history part' and 'has right process boundary' some ('process boundary' and ('part of' some 'enrollment in an insurance policy period')) date - 'temporal interval' and inverse 'is duration of' some ('measurement datum' and 'has value specification' some ('scalar value specification' and 'has value' '1' and 'has measurement unit label' 'day') PCORNet Enrollment \"ENROLLMENT Domain Description: Enrollment is a concept that defines a period of time during which all medically-attended events are expected to be observed. This concept is often insurance-based, but other methods of defining enrollment are possible.\" \"The ENROLLMENT table contains one record per unique combination of PATID, ENR_START_DATE, and BASIS. * What are \"medically-attended events\"? * The enrollment dates specify a period of complete data capture. This is a different notion from enrollment in an insurance plan although enrollment in an insurance plan can be the \"basis\" of the complete data capture. Also, notice that the purpose of enrollment dates is to support a closed-world assumption. (\"The ENROLLMENT table provides an important analytic basis for identifying periods during which medical care should be observed, for calculating person-time, and for inferring the meaning of unobserved care (ie, if care is not observed, it likely did not happen).\")","title":"Insurance"},{"location":"modeling/Insurance/#new-terms","text":"contract - Superclass: Document and (is_specified_output_of some 'document act') indemnity contract - Superclass: contract and ('has part' some 'action specification') and ('has part' some 'objective specification') and ('has part' some 'plan specification') and ('has part' some 'conditional specification') insurance policy - Superclass: 'indemnity contract' and is_specified_output_of some ('document act' and (has_agent some 'insurance company') and (has_agent some ('collection of humans' and 'has member' only (bearer_of some 'policy holder role')))) ['and has_specified_output some 'insured party role'' should modify 'document act', but this is not possible while declaration is defined as having a SLGDC as specified output since roles are not generically dependent.] insured party role - Superclass: 'role in human social processes' and (inverse 'is about' some ('conditional specification' and 'part of' some 'insurance policy')) and (is_specified_output_of some ('document act' and (has_specified_output some 'insurance policy'))) insurance company - Superclass: organization and ('bearer of' some ('payer role' and (concretizes some ('socio-legal generically dependent continuant' and is_specified_output_of some 'document act')))) and ('bearer of' some 'party to an insurance policy') and (inverse 'is about' some ('action specification' and 'part of' some 'insurance policy')) policy holder role - Superclass: 'insured party role' and ('inheres in' some ('bearer of' some 'party to an insurance policy')) payer role - Superclass: 'role in human social processes' party to an insurance policy - Superclass: 'party to a legal agreement' enrollment in an insurance policy period - Subclass: 'temporal interval' and (inverse(exists at) ('insured part role' and 'inheres in' (some organism))) and ('part of' some ('temporal interval' and 'is temporal location of' [life of an organism]) enrollment start date - date and 'is occupied by' some ('history part' and 'has left process boundary' some ('process boundary' and ('part of' some 'enrollment in an insurance policy period')) enrollment end date - date and 'is occupied by' some ('history part' and 'has right process boundary' some ('process boundary' and ('part of' some 'enrollment in an insurance policy period')) date - 'temporal interval' and inverse 'is duration of' some ('measurement datum' and 'has value specification' some ('scalar value specification' and 'has value' '1' and 'has measurement unit label' 'day')","title":"New Terms"},{"location":"modeling/Insurance/#pcornet-enrollment","text":"\"ENROLLMENT Domain Description: Enrollment is a concept that defines a period of time during which all medically-attended events are expected to be observed. This concept is often insurance-based, but other methods of defining enrollment are possible.\" \"The ENROLLMENT table contains one record per unique combination of PATID, ENR_START_DATE, and BASIS. * What are \"medically-attended events\"? * The enrollment dates specify a period of complete data capture. This is a different notion from enrollment in an insurance plan although enrollment in an insurance plan can be the \"basis\" of the complete data capture. Also, notice that the purpose of enrollment dates is to support a closed-world assumption. (\"The ENROLLMENT table provides an important analytic basis for identifying periods during which medical care should be observed, for calculating person-time, and for inferring the meaning of unobserved care (ie, if care is not observed, it likely did not happen).\")","title":"PCORNet Enrollment"},{"location":"modeling/Modeling-Discharge-and-Discharge-Status/","text":"Discharge as Document Act A discharge is a document act that involves a document that concretizes a directive information entity. That directive information entity has as parts a plan specification, an action specification, and an objective specification. Usually the objective specification is about where the patient will go upon discharge, e.g., home, a rehab facility, etc. Discharges have as agents humans who are the bearer of a health care provider role.","title":"Discharge as Document Act"},{"location":"modeling/Modeling-Discharge-and-Discharge-Status/#discharge-as-document-act","text":"A discharge is a document act that involves a document that concretizes a directive information entity. That directive information entity has as parts a plan specification, an action specification, and an objective specification. Usually the objective specification is about where the patient will go upon discharge, e.g., home, a rehab facility, etc. Discharges have as agents humans who are the bearer of a health care provider role.","title":"Discharge as Document Act"},{"location":"modeling/OMRSE-Language-and-Language-Individuals/","text":"OMRSE Language Modules We created omrse-language.owl to represent 'language' and 'preferred language content entity', among other language-related classes. OMRSE Language Individuals We represent individual languages as OWL individuals in language-individuals.owl. This file was programmatically generated using the Python 3 rdflib (v4.2.2) library. The information contained in the annotations for each language individual was pulled directly from the table in the \"List of ISO 639-2 codes\" Wikipedia page , although not exhaustively. In particular, we excluded any languages from that table that were categorized in the \"type\" column as \"Collective,\" \"Special,\" or \"Local.\" The rdfs:label of each language individual comes from the ISO 639-2 language name (i.e., the English name(s) of the language). In some instances, there are multiple names listed, in which case we used the first name listed as the rdfs:label and created alternative term annotations for each additional name. ISO 639-1 annotation The first part of the ISO 639 standard--ISO 639-1--consists of two-letter codes that are derived from the native name of each language covered. In total, there are 184 ISO 639-1 codes, which amounts to less than half of the languages that are represented in OMRSE Language. Therefore, we only include ISO 639-1 annotations for those languages that have been assigned one. ISO 639-2(T/B) annotations The second part of the ISO 639 standard--ISO 639-2--contains of three-letter language codes for the names of each language. This standard consists of two sets of codes--the bibliographic set (ISO 639-2/B) and the terminological set (ISO 639-2/T). The difference between the two is that the three-letter codes in the bibliographic set were derived from the English name of the language, while the three-letter codes in the terminological set were derived from the native name. In total, however, there are only ~20 languages in ISO 639-2 whose three-letter ISO 639-2/T codes differ from the ISO 639-2/B. Each language individual contains both an ISO 639-2/T and an ISO 639-2/B annotation. Native Name annotation Where available, we created separate alternative term annotations for the native name(s) of a language using the native names listed in the aforementioned Wikipedia table. If the language was assigned a two-digit ISO 639-1 code, we assigned it to the language tag of each such annotation. If no ISO 639-1 code was available, we left the language tag blank.","title":"OMRSE Language Modules"},{"location":"modeling/OMRSE-Language-and-Language-Individuals/#omrse-language-modules","text":"We created omrse-language.owl to represent 'language' and 'preferred language content entity', among other language-related classes.","title":"OMRSE Language Modules"},{"location":"modeling/OMRSE-Language-and-Language-Individuals/#omrse-language-individuals","text":"We represent individual languages as OWL individuals in language-individuals.owl. This file was programmatically generated using the Python 3 rdflib (v4.2.2) library. The information contained in the annotations for each language individual was pulled directly from the table in the \"List of ISO 639-2 codes\" Wikipedia page , although not exhaustively. In particular, we excluded any languages from that table that were categorized in the \"type\" column as \"Collective,\" \"Special,\" or \"Local.\" The rdfs:label of each language individual comes from the ISO 639-2 language name (i.e., the English name(s) of the language). In some instances, there are multiple names listed, in which case we used the first name listed as the rdfs:label and created alternative term annotations for each additional name.","title":"OMRSE Language Individuals"},{"location":"modeling/OMRSE-Language-and-Language-Individuals/#iso-639-1-annotation","text":"The first part of the ISO 639 standard--ISO 639-1--consists of two-letter codes that are derived from the native name of each language covered. In total, there are 184 ISO 639-1 codes, which amounts to less than half of the languages that are represented in OMRSE Language. Therefore, we only include ISO 639-1 annotations for those languages that have been assigned one.","title":"ISO 639-1 annotation"},{"location":"modeling/OMRSE-Language-and-Language-Individuals/#iso-639-2tb-annotations","text":"The second part of the ISO 639 standard--ISO 639-2--contains of three-letter language codes for the names of each language. This standard consists of two sets of codes--the bibliographic set (ISO 639-2/B) and the terminological set (ISO 639-2/T). The difference between the two is that the three-letter codes in the bibliographic set were derived from the English name of the language, while the three-letter codes in the terminological set were derived from the native name. In total, however, there are only ~20 languages in ISO 639-2 whose three-letter ISO 639-2/T codes differ from the ISO 639-2/B. Each language individual contains both an ISO 639-2/T and an ISO 639-2/B annotation.","title":"ISO 639-2(T/B) annotations"},{"location":"modeling/OMRSE-Language-and-Language-Individuals/#native-name-annotation","text":"Where available, we created separate alternative term annotations for the native name(s) of a language using the native names listed in the aforementioned Wikipedia table. If the language was assigned a two-digit ISO 639-1 code, we assigned it to the language tag of each such annotation. If no ISO 639-1 code was available, we left the language tag blank.","title":"Native Name annotation"},{"location":"modeling/OMRSE-Overview/","text":"Welcome to the OMRSE wiki! The Ontology for Modeling and Representation of Social Entities or OMRSE (previously known as the Ontology of Medically Related Social Entities) is an ontology that represents various roles, processes, and dependent entities that are the product of interactions among humans (although we haven't ruled out social interactions of other species). It began by representing various roles required for demographics, including those required to represent gender and marital status. We welcome all contributions to OMRSE! The permanent URL to the latest, release version of OMRSE is http://purl.obolibrary.org/obo/omrse.owl","title":"OMRSE Overview"},{"location":"modeling/PCORNet-Smoking-Status/","text":"PCORNet VITAL Table Specification 01 = Current every day smoker 02 = Current some day smoker 03 = Former smoker 04 = Never smoker 05 = Smoker, current status unknown 06 = Unknown if ever smoked 07 = Heavy tobacco smoker 08 = Light tobacco smoker NI = No information UN = Unknown OT = Other \"This field is new to v3.0. Indicator for any form of tobacco that is smoked. Per Meaningful Use guidance, \u201c\u2026smoking status includes any form of tobacco that is smoked, but not all tobacco use.\u201d \u201c\u2019Light smoker\u2019 is interpreted to mean less than 10 cigarettes per day, or an equivalent (but less concretely defined) quantity of cigar or pipe smoke. \u2018Heavy smoker\u2019 is interpreted to mean greater than 10 cigarettes per day or an equivalent (but less concretely defined) quantity of cigar or pipe smoke.\u201d \u201c\u2026we understand that a \u201ccurrent every day smoker\u201d or \u201ccurrent some day smoker\u201d is an individual who has smoked at least 100 cigarettes during his/her lifetime and still regularly smokes every day or periodically, yet consistently; a \u201cformer smoker\u201d would be an individual who has smoked at least 100 cigarettes during his/her lifetime but does not currently smoke; and a \u201cnever smoker\u201d would be an individual who has not smoked 100 or more cigarettes during his/her lifetime.\u201d \" Questions Is a former smoker a smoker?","title":"PCORNet VITAL Table Specification"},{"location":"modeling/PCORNet-Smoking-Status/#pcornet-vital-table-specification","text":"01 = Current every day smoker 02 = Current some day smoker 03 = Former smoker 04 = Never smoker 05 = Smoker, current status unknown 06 = Unknown if ever smoked 07 = Heavy tobacco smoker 08 = Light tobacco smoker NI = No information UN = Unknown OT = Other \"This field is new to v3.0. Indicator for any form of tobacco that is smoked. Per Meaningful Use guidance, \u201c\u2026smoking status includes any form of tobacco that is smoked, but not all tobacco use.\u201d \u201c\u2019Light smoker\u2019 is interpreted to mean less than 10 cigarettes per day, or an equivalent (but less concretely defined) quantity of cigar or pipe smoke. \u2018Heavy smoker\u2019 is interpreted to mean greater than 10 cigarettes per day or an equivalent (but less concretely defined) quantity of cigar or pipe smoke.\u201d \u201c\u2026we understand that a \u201ccurrent every day smoker\u201d or \u201ccurrent some day smoker\u201d is an individual who has smoked at least 100 cigarettes during his/her lifetime and still regularly smokes every day or periodically, yet consistently; a \u201cformer smoker\u201d would be an individual who has smoked at least 100 cigarettes during his/her lifetime but does not currently smoke; and a \u201cnever smoker\u201d would be an individual who has not smoked 100 or more cigarettes during his/her lifetime.\u201d \"","title":"PCORNet VITAL Table Specification"},{"location":"modeling/PCORNet-Smoking-Status/#questions","text":"Is a former smoker a smoker?","title":"Questions"},{"location":"modeling/Race-And-Ethnicity/","text":"Race And Ethnicity in the US Census Resources OMB https://www.whitehouse.gov/omb/fedreg_race-ethnicity https://www.whitehouse.gov/sites/default/files/omb/assets/information_and_regulatory_affairs/re_app-a-update.pdf History of the Census and Other Race and Ethnicity Classification Schemes http://www.pewsocialtrends.org/2010/01/21/race-and-the-census-the-%E2%80%9Cnegro%E2%80%9D-controversy/ Option #1 for modeling race data gathered by the US Census OBM Categories to Date American Indian or Alaska Native. A person having origins in any of the original peoples of North and South America (including Central America), and who maintains tribal affiliation or community attachment. Asian. A person having origins in any of the original peoples of the Far East, Southeast Asia, or the Indian subcontinent including, for example, Cambodia, China, India, Japan, Korea, Malaysia, Pakistan, the Philippine Islands, Thailand, and Vietnam. Black or African American. A person having origins in any of the black racial groups of Africa. Terms such as \u201cHaitian\u201d or \u201cNegro\u201d can be used in addition to \u201cBlack or African American.\u201d Native Hawaiian or Other Pacific Islander. A person having origins in any of the original peoples of Hawaii, Guam, Samoa, or other Pacific Islands. White. A person having origins in any of the original peoples of Europe, the Middle East, or North Africa. How to model self-identity claims about race and ethnicity Self-identity claims about race and ethnicity as they appear in the US Census are Information Content Entities that are intended to be a truthful statement. They are part of a 'documented identity'. A documented identity is \"the aggregate of all data items about an entity. Notice that a documented identity is not itself a document since a document is intended to be understood as a whole and data items about an individual are usually scattered across different documents.\" Racial identities are part of document identities and are about a person. They have parts some specified output of a racial identification process . Racial identification processes are social acts and planned processes. A special case of a racial identification process is a US Census Racial identity process, which is also part of a US Census Survey and has specified output (at present) some OMB racial identification. Racial identities are self-identified when they are about the same person who is the agent of the racial identification process. This can not be represented in OWL/DL but can be captured by a SPARQL query. New classes for OMRSE racial identity - Superclass: 'information content entity' and ('is about' some 'Homo sapiens') and ('part of' some 'documented identity') and ('has part' some (is_specified_output_of some 'racial identification process')) and ('part of' some 'documented identity') racial identification process - Superclass: 'social act' and 'planned process' and ('has specified output' some ('part of' some 'racial identity')) ethnic identity - Superclass: 'information content entity' and ('is about' some 'Homo sapiens') and ('part of' some 'documented identity') and ('has part' some (is_specified_output_of some 'ethnic identification process')) and ('part of' some 'documented identity') ethnic identification process - Superclass: 'social act' and 'planned process' and ('has specified output' some ('part of' some 'ethnic identity'))","title":"Race And Ethnicity in the US Census"},{"location":"modeling/Race-And-Ethnicity/#race-and-ethnicity-in-the-us-census","text":"Resources OMB https://www.whitehouse.gov/omb/fedreg_race-ethnicity https://www.whitehouse.gov/sites/default/files/omb/assets/information_and_regulatory_affairs/re_app-a-update.pdf History of the Census and Other Race and Ethnicity Classification Schemes http://www.pewsocialtrends.org/2010/01/21/race-and-the-census-the-%E2%80%9Cnegro%E2%80%9D-controversy/","title":"Race And Ethnicity in the US Census"},{"location":"modeling/Race-And-Ethnicity/#option-1-for-modeling-race-data-gathered-by-the-us-census","text":"","title":"Option #1 for modeling race data gathered by the US Census"},{"location":"modeling/Race-And-Ethnicity/#obm-categories-to-date","text":"American Indian or Alaska Native. A person having origins in any of the original peoples of North and South America (including Central America), and who maintains tribal affiliation or community attachment. Asian. A person having origins in any of the original peoples of the Far East, Southeast Asia, or the Indian subcontinent including, for example, Cambodia, China, India, Japan, Korea, Malaysia, Pakistan, the Philippine Islands, Thailand, and Vietnam. Black or African American. A person having origins in any of the black racial groups of Africa. Terms such as \u201cHaitian\u201d or \u201cNegro\u201d can be used in addition to \u201cBlack or African American.\u201d Native Hawaiian or Other Pacific Islander. A person having origins in any of the original peoples of Hawaii, Guam, Samoa, or other Pacific Islands. White. A person having origins in any of the original peoples of Europe, the Middle East, or North Africa.","title":"OBM Categories to Date"},{"location":"modeling/Race-And-Ethnicity/#how-to-model-self-identity-claims-about-race-and-ethnicity","text":"Self-identity claims about race and ethnicity as they appear in the US Census are Information Content Entities that are intended to be a truthful statement. They are part of a 'documented identity'. A documented identity is \"the aggregate of all data items about an entity. Notice that a documented identity is not itself a document since a document is intended to be understood as a whole and data items about an individual are usually scattered across different documents.\" Racial identities are part of document identities and are about a person. They have parts some specified output of a racial identification process . Racial identification processes are social acts and planned processes. A special case of a racial identification process is a US Census Racial identity process, which is also part of a US Census Survey and has specified output (at present) some OMB racial identification. Racial identities are self-identified when they are about the same person who is the agent of the racial identification process. This can not be represented in OWL/DL but can be captured by a SPARQL query.","title":"How to model self-identity claims about race and ethnicity"},{"location":"modeling/Race-And-Ethnicity/#new-classes-for-omrse","text":"racial identity - Superclass: 'information content entity' and ('is about' some 'Homo sapiens') and ('part of' some 'documented identity') and ('has part' some (is_specified_output_of some 'racial identification process')) and ('part of' some 'documented identity') racial identification process - Superclass: 'social act' and 'planned process' and ('has specified output' some ('part of' some 'racial identity')) ethnic identity - Superclass: 'information content entity' and ('is about' some 'Homo sapiens') and ('part of' some 'documented identity') and ('has part' some (is_specified_output_of some 'ethnic identification process')) and ('part of' some 'documented identity') ethnic identification process - Superclass: 'social act' and 'planned process' and ('has specified output' some ('part of' some 'ethnic identity'))","title":"New classes for OMRSE"},{"location":"modeling/Representing-Languages-as-codified-by--ISO-639/","text":"Languages and ISO-639 Background The need for languages as a class arises from our work representing demographic information in Continuity of Care Documents (CCDs). Preferred language is an element in the CCD, and the header has meta-data indicating the language of the CCD. More details are in the following slide deck (slides 10-17): http://ncor.buffalo.edu/2015/CTSO/Hogan.pptx In these slides we propose introducing directive information entities (DIEs) for languages, e.g., 'English Directive Information Entity'. These DIEs might be concretized by languages considered as dispositions. Note that the slides refer to MF_00000022, but this has been removed from the Mental Functioning Ontology. https://github.com/jannahastings/mental-functioning-ontology/commit/6a1231b8d973c59cb6189a6d6750632c70fdcf95 Spoken versus written language and language as a directive information entity The Mental Health Functioning Ontology has a class with the label 'language' that are subclasses of dispositions, but this refers to human spoken language. ISO-639 was initially developed to describe literature, so way may consider creating a new class to allign ISO language codes. If we do not represent ISO codes for language in terms of MF's class for language, language DIEs could still be concretized by MFs. Ontological considerations for modeling language (1) \u2018Language\u2019 is ambiguous between (a) the capacity for communicating with language (disposition) (b) a specific language such as English (which we propose modeling as a directive information entity) 'Person A speaks English' entails that Person A has a disposition to communicate with language (any language) and also that they have a disposition to communicate with speech, written language, or other signs that concretize an English DIE. We could then model the language of a document in terms of the language DIE, but the details here still need to be worked out. Are written documents and spoken pieces of language concretization of ICEs that are the outputs of processes that realize the language disposition? (2) Is \u2018English DIE\u2019 an individual or a class? (a) How should we model dialects? Some proposed necessary conditions for preferred languages A Patient Whose Preferred Language is English Subclass Of: member of the population that is a bearer of a language disposition that concretizes an/the English DIE. Person 2 speaks Englis.h \u2018English DIE\u2019 \u2018is concretized by\u2019 some (language and \u2018inheres in at all times\u2019 some population and (\u2018has continuant part at all times\u2019 (\u2018Person 2\u2019 \u2018is a\u2019 (human being))))","title":"Languages and ISO-639"},{"location":"modeling/Representing-Languages-as-codified-by--ISO-639/#languages-and-iso-639","text":"","title":"Languages and ISO-639"},{"location":"modeling/Representing-Languages-as-codified-by--ISO-639/#background","text":"The need for languages as a class arises from our work representing demographic information in Continuity of Care Documents (CCDs). Preferred language is an element in the CCD, and the header has meta-data indicating the language of the CCD. More details are in the following slide deck (slides 10-17): http://ncor.buffalo.edu/2015/CTSO/Hogan.pptx In these slides we propose introducing directive information entities (DIEs) for languages, e.g., 'English Directive Information Entity'. These DIEs might be concretized by languages considered as dispositions. Note that the slides refer to MF_00000022, but this has been removed from the Mental Functioning Ontology. https://github.com/jannahastings/mental-functioning-ontology/commit/6a1231b8d973c59cb6189a6d6750632c70fdcf95","title":"Background"},{"location":"modeling/Representing-Languages-as-codified-by--ISO-639/#spoken-versus-written-language-and-language-as-a-directive-information-entity","text":"The Mental Health Functioning Ontology has a class with the label 'language' that are subclasses of dispositions, but this refers to human spoken language. ISO-639 was initially developed to describe literature, so way may consider creating a new class to allign ISO language codes. If we do not represent ISO codes for language in terms of MF's class for language, language DIEs could still be concretized by MFs.","title":"Spoken versus written language and language as a directive information entity"},{"location":"modeling/Representing-Languages-as-codified-by--ISO-639/#ontological-considerations-for-modeling-language","text":"(1) \u2018Language\u2019 is ambiguous between (a) the capacity for communicating with language (disposition) (b) a specific language such as English (which we propose modeling as a directive information entity) 'Person A speaks English' entails that Person A has a disposition to communicate with language (any language) and also that they have a disposition to communicate with speech, written language, or other signs that concretize an English DIE. We could then model the language of a document in terms of the language DIE, but the details here still need to be worked out. Are written documents and spoken pieces of language concretization of ICEs that are the outputs of processes that realize the language disposition? (2) Is \u2018English DIE\u2019 an individual or a class? (a) How should we model dialects?","title":"Ontological considerations for modeling language"},{"location":"modeling/Representing-Languages-as-codified-by--ISO-639/#some-proposed-necessary-conditions-for-preferred-languages","text":"","title":"Some proposed necessary conditions for preferred languages"},{"location":"modeling/Representing-Languages-as-codified-by--ISO-639/#a-patient-whose-preferred-language-is-english","text":"Subclass Of: member of the population that is a bearer of a language disposition that concretizes an/the English DIE.","title":"A Patient Whose Preferred Language is English"},{"location":"modeling/Representing-Languages-as-codified-by--ISO-639/#person-2-speaks-english","text":"\u2018English DIE\u2019 \u2018is concretized by\u2019 some (language and \u2018inheres in at all times\u2019 some population and (\u2018has continuant part at all times\u2019 (\u2018Person 2\u2019 \u2018is a\u2019 (human being))))","title":"Person 2 speaks Englis.h"},{"location":"modeling/Typology-of-Health-Care-Facilities/","text":"General Strategy for Modeling Health Care Facilities A facility is defined as \"an architectural structure that is the bearer of some function.\" 'hospital facility' has the following DL restriction: facility and (('is owned by' some 'hospital organization') or ('is administrated by' some 'hospital organization')) and (bearer_of some 'hospital function') This definition ties the facility to an organization in addition to a function. Question: Do we need to tie an organization to each of the types of health care facilities? At a minimum we could specify that are owned or administered by health care provider organizations. Types of Health Care Facilities From PCORNet CDM DISCHARGE_STATUS Urgent care Ambulatory surgery Hospice Emergency department Physician office Outpatient clinic Overnight dialysis Rehabilitation Skilled nursing Residential Nursing Home Differentiation of Facilities Facilities are material entities that are differentiated according to the functions they bear and the organizations that own or administer them. Proposed Natural Language Definitions for Functions Urgent care function - \"A function inhering in a material entity that is realized by the material entity being the site at which outpatient healthcare is provided for illness or injury that requires immediate care but does not require a visit to an emergency department.\" Ambulatory surgery function - \"A function inhering in a material entity that is realized by the material entity being the site at which outpatient surgical care is provided to a patient population.\" Hospice function - \"A function inhering in a material entity that is realized by the material entity being the site at which inpatient palliative healthcare is provided to a patient population with a terminal prognosis.\" Emergency department function - \"A function inhering in a material entity that is realized by the material entity being the site at which emergency medicine and treatment of acute illness and injury is provided to a patient population.\" Physician office function - in progress Outpatient clinic function - \"A function inhering in a material entity that is realized by the material entity being the site at which medical care is provided to a patient population and in which the patients receiving the medical care each stay for less than 24 hours.\" Overnight dialysis clinic function - \"A function inhering in a material entity that is realized by the material entity being the site at which hemodialysis is administered to a patient population at night or when the patient habitually sleeps.\" Rehabilitation facility function - in progress Skilled nursing facility function - in progress Residential facility function - in progress Nursing home function - in progress Criteria of Differentiation of Functions Outpatient, Inpatient, ER - in progress Type of Care - in progress Temporary or Permanent Living Arrangement - in progress Other relevant terms Skilled Nursing - in progress Custodial Care - in progress","title":"General Strategy for Modeling Health Care Facilities"},{"location":"modeling/Typology-of-Health-Care-Facilities/#general-strategy-for-modeling-health-care-facilities","text":"A facility is defined as \"an architectural structure that is the bearer of some function.\" 'hospital facility' has the following DL restriction: facility and (('is owned by' some 'hospital organization') or ('is administrated by' some 'hospital organization')) and (bearer_of some 'hospital function') This definition ties the facility to an organization in addition to a function. Question: Do we need to tie an organization to each of the types of health care facilities? At a minimum we could specify that are owned or administered by health care provider organizations.","title":"General Strategy for Modeling Health Care Facilities"},{"location":"modeling/Typology-of-Health-Care-Facilities/#types-of-health-care-facilities-from-pcornet-cdm-discharge_status","text":"Urgent care Ambulatory surgery Hospice Emergency department Physician office Outpatient clinic Overnight dialysis Rehabilitation Skilled nursing Residential Nursing Home","title":"Types of Health Care Facilities From PCORNet CDM DISCHARGE_STATUS"},{"location":"modeling/Typology-of-Health-Care-Facilities/#differentiation-of-facilities","text":"Facilities are material entities that are differentiated according to the functions they bear and the organizations that own or administer them.","title":"Differentiation of Facilities"},{"location":"modeling/Typology-of-Health-Care-Facilities/#proposed-natural-language-definitions-for-functions","text":"Urgent care function - \"A function inhering in a material entity that is realized by the material entity being the site at which outpatient healthcare is provided for illness or injury that requires immediate care but does not require a visit to an emergency department.\" Ambulatory surgery function - \"A function inhering in a material entity that is realized by the material entity being the site at which outpatient surgical care is provided to a patient population.\" Hospice function - \"A function inhering in a material entity that is realized by the material entity being the site at which inpatient palliative healthcare is provided to a patient population with a terminal prognosis.\" Emergency department function - \"A function inhering in a material entity that is realized by the material entity being the site at which emergency medicine and treatment of acute illness and injury is provided to a patient population.\" Physician office function - in progress Outpatient clinic function - \"A function inhering in a material entity that is realized by the material entity being the site at which medical care is provided to a patient population and in which the patients receiving the medical care each stay for less than 24 hours.\" Overnight dialysis clinic function - \"A function inhering in a material entity that is realized by the material entity being the site at which hemodialysis is administered to a patient population at night or when the patient habitually sleeps.\" Rehabilitation facility function - in progress Skilled nursing facility function - in progress Residential facility function - in progress Nursing home function - in progress","title":"Proposed Natural Language Definitions for Functions"},{"location":"modeling/Typology-of-Health-Care-Facilities/#criteria-of-differentiation-of-functions","text":"Outpatient, Inpatient, ER - in progress Type of Care - in progress Temporary or Permanent Living Arrangement - in progress","title":"Criteria of Differentiation of Functions"},{"location":"modeling/Typology-of-Health-Care-Facilities/#other-relevant-terms","text":"Skilled Nursing - in progress Custodial Care - in progress","title":"Other relevant terms"},{"location":"modeling/What-is-the-relation-between-a-role-and-the-organization-that-creates-sanctions-confers-it/","text":"Per BFO, roles inhere in continuants and are realized by processes. However, we often need to connect a continuant that is the bearer of a role to the organization, government, etc. that confers the role. For example, how does one relate an employee to her employer organization? First, we note that BFO2 currently posits that no relational roles exist, so employer/employee are not connected by jointly bearing a single role: \"Hypothesis: There are no relational roles. In other words, each role is the role of exactly one bearer.\" Option #1: The connection is through participation in a role-conferring process One possibility is that both the bestower of the role, and the recipient of the role participate in a process whereby the role is conferred to the recipient, and after which the recipient is the bearer of the role: Consider the following particulars: o1: organization #1 p1: person #1 r1: p1's role as employee of o1 h1: hiring process of p1 by o1 Then, we say that: o1 'is participant of' h1 (note, it's really certain other employees of o1 that participate) p1 'is participant of' h1 r1 'begins to exist during' h1 The key issue is that not all roles are conferred by such processes. Option #2: The connection is through Socio-legal dependent continuants and the declarations for which the SLGDC is a specified output. Definition of SLGDC - Socio-legal generically dependent continuants are generically dependent continuants that come into existence through declarations and are concretized as roles. Definition of 'declaration' - A social act that brings about, transfers or revokes a socio-legal generically dependent continuant. Declarations do not depend on words spoken or written, but sometimes are merely actions, for instance the signing of a document. Class restriction of 'declaration' - (('legally revokes' some 'socio-legal generically dependent continuant') or ('legally transfers' some 'socio-legal generically dependent continuant') or (has_specified_output some 'socio-legal generically dependent continuant')) and (realizes some 'declaration performer role') and (has_agent some (('Homo sapiens' or organization or 'collection of humans' or 'aggregate of organizations') and (bearer_of some 'declaration performer role'))) The gist - Some roles are concretizations of SLGDCs. An SLGDC is created by a declaration that involves some agent (the bearer of a declaration performer role) and a declaration target (the bearer of the role that concretizes the SLGDC). An employee role is a concretization of a SLGDC. An employee is the bearer of a role that concretizes an SLGDC that is the specified output of a declaration that had the employer as agent. SLGDCs can be distinguished according according to a couple criteria: the type of entity that is the agent (e.g., Human or organization) and the type of entity that is the declaration target (e.g., human or organization). Roles that concretize SLGDCs can be distinguished in the same way. In the case of employee/employer scenarios, the declaration target (or the bearer of the employee role that concretizes an SLGCD) is a human and the agent is an organization. With this in mind I suggest that we have the following hierarchy in OMRSE: 'role in human social processes' 'organism social role' 'human social role' 'socio-legal human social role' (is_concretization_of some ('socio-legal generically dependent continuant')) 'human role within an organization' (is_concretization_of some ('socio-legal generically dependent continuant' and is_specified_output_of some (declaration and has_agent some (organization or 'aggregate of organizations'))) 'student role' 'employee role'","title":"What is the relation between a role and the organization that creates sanctions confers it"},{"location":"modeling/What-is-the-relation-between-a-role-and-the-organization-that-creates-sanctions-confers-it/#option-1-the-connection-is-through-participation-in-a-role-conferring-process","text":"One possibility is that both the bestower of the role, and the recipient of the role participate in a process whereby the role is conferred to the recipient, and after which the recipient is the bearer of the role: Consider the following particulars: o1: organization #1 p1: person #1 r1: p1's role as employee of o1 h1: hiring process of p1 by o1 Then, we say that: o1 'is participant of' h1 (note, it's really certain other employees of o1 that participate) p1 'is participant of' h1 r1 'begins to exist during' h1 The key issue is that not all roles are conferred by such processes.","title":"Option #1: The connection is through participation in a role-conferring process"},{"location":"modeling/What-is-the-relation-between-a-role-and-the-organization-that-creates-sanctions-confers-it/#option-2-the-connection-is-through-socio-legal-dependent-continuants-and-the-declarations-for-which-the-slgdc-is-a-specified-output","text":"Definition of SLGDC - Socio-legal generically dependent continuants are generically dependent continuants that come into existence through declarations and are concretized as roles. Definition of 'declaration' - A social act that brings about, transfers or revokes a socio-legal generically dependent continuant. Declarations do not depend on words spoken or written, but sometimes are merely actions, for instance the signing of a document. Class restriction of 'declaration' - (('legally revokes' some 'socio-legal generically dependent continuant') or ('legally transfers' some 'socio-legal generically dependent continuant') or (has_specified_output some 'socio-legal generically dependent continuant')) and (realizes some 'declaration performer role') and (has_agent some (('Homo sapiens' or organization or 'collection of humans' or 'aggregate of organizations') and (bearer_of some 'declaration performer role'))) The gist - Some roles are concretizations of SLGDCs. An SLGDC is created by a declaration that involves some agent (the bearer of a declaration performer role) and a declaration target (the bearer of the role that concretizes the SLGDC). An employee role is a concretization of a SLGDC. An employee is the bearer of a role that concretizes an SLGDC that is the specified output of a declaration that had the employer as agent. SLGDCs can be distinguished according according to a couple criteria: the type of entity that is the agent (e.g., Human or organization) and the type of entity that is the declaration target (e.g., human or organization). Roles that concretize SLGDCs can be distinguished in the same way. In the case of employee/employer scenarios, the declaration target (or the bearer of the employee role that concretizes an SLGCD) is a human and the agent is an organization. With this in mind I suggest that we have the following hierarchy in OMRSE: 'role in human social processes' 'organism social role' 'human social role' 'socio-legal human social role' (is_concretization_of some ('socio-legal generically dependent continuant')) 'human role within an organization' (is_concretization_of some ('socio-legal generically dependent continuant' and is_specified_output_of some (declaration and has_agent some (organization or 'aggregate of organizations'))) 'student role' 'employee role'","title":"Option #2: The connection is through Socio-legal dependent continuants and the declarations for which the SLGDC is a specified output."},{"location":"odk-workflows/","text":"Default ODK Workflows Daily Editors Workflow Release Workflow Manage your ODK Repository Setting up Docker for ODK Imports management Managing the documentation Managing your Automated Testing","title":"Overview"},{"location":"odk-workflows/#default-odk-workflows","text":"Daily Editors Workflow Release Workflow Manage your ODK Repository Setting up Docker for ODK Imports management Managing the documentation Managing your Automated Testing","title":"Default ODK Workflows"},{"location":"odk-workflows/ContinuousIntegration/","text":"Introduction to Continuous Integration Workflows with ODK Historically, most repos have been using Travis CI for continuous integration testing and building, but due to runtime restrictions, we recently switched a lot of our repos to GitHub actions. You can set up your repo with CI by adding this to your configuration file (src/ontology/omrse-odk.yaml): ci: - github_actions When updateing your repo , you will notice a new file being added: .github/workflows/qc.yml . This file contains your CI logic, so if you need to change, or add anything, this is the place! Alternatively, if your repo is in GitLab instead of GitHub, you can set up your repo with GitLab CI by adding this to your configuration file (src/ontology/omrse-odk.yaml): ci: - gitlab-ci This will add a file called .gitlab-ci.yml in the root of your repo.","title":"Continuous Integration"},{"location":"odk-workflows/ContinuousIntegration/#introduction-to-continuous-integration-workflows-with-odk","text":"Historically, most repos have been using Travis CI for continuous integration testing and building, but due to runtime restrictions, we recently switched a lot of our repos to GitHub actions. You can set up your repo with CI by adding this to your configuration file (src/ontology/omrse-odk.yaml): ci: - github_actions When updateing your repo , you will notice a new file being added: .github/workflows/qc.yml . This file contains your CI logic, so if you need to change, or add anything, this is the place! Alternatively, if your repo is in GitLab instead of GitHub, you can set up your repo with GitLab CI by adding this to your configuration file (src/ontology/omrse-odk.yaml): ci: - gitlab-ci This will add a file called .gitlab-ci.yml in the root of your repo.","title":"Introduction to Continuous Integration Workflows with ODK"},{"location":"odk-workflows/EditorsWorkflow/","text":"Editors Workflow The editors workflow is one of the formal workflows to ensure that the ontology is developed correctly according to ontology engineering principles. There are a few different editors workflows: Local editing workflow: Editing the ontology in your local environment by hand, using tools such as Prot\u00e9g\u00e9, ROBOT templates or DOSDP patterns. Completely automated data pipeline (GitHub Actions) DROID workflow This document only covers the first editing workflow, but more will be added in the future Local editing workflow Workflow requirements: git github docker editing tool of choice, e.g. Prot\u00e9g\u00e9, your favourite text editor, etc 1. Create issue Ensure that there is a ticket on your issue tracker that describes the change you are about to make. While this seems optional, this is a very important part of the social contract of building an ontology - no change to the ontology should be performed without a good ticket, describing the motivation and nature of the intended change. 2. Update main branch In your local environment (e.g. your laptop), make sure you are on the main (prev. master ) branch and ensure that you have all the upstream changes, for example: git checkout main git pull 3. Create feature branch Create a new branch. Per convention, we try to use meaningful branch names such as: - issue23removeprocess (where issue 23 is the related issue on GitHub) - issue26addcontributor - release20210101 (for releases) On your command line, this looks like this: git checkout -b issue23removeprocess 4. Perform edit Using your editor of choice, perform the intended edit. For example: Prot\u00e9g\u00e9 Open src/ontology/omrse-edit.owl in Prot\u00e9g\u00e9 Make the change Save the file TextEdit Open src/ontology/omrse-edit.owl in TextEdit (or Sublime, Atom, Vim, Nano) Make the change Save the file Consider the following when making the edit. According to our development philosophy, the only places that should be manually edited are: src/ontology/omrse-edit.owl Any ROBOT templates you chose to use (the TSV files only) Any DOSDP data tables you chose to use (the TSV files, and potentially the associated patterns) components (anything in src/ontology/components ), see here . Imports should not be edited (any edits will be flushed out with the next update). However, refreshing imports is a potentially breaking change - and is discussed elsewhere . Changes should usually be small. Adding or changing 1 term is great. Adding or changing 10 related terms is ok. Adding or changing 100 or more terms at once should be considered very carefully. 4. Check the Git diff This step is very important. Rather than simply trusting your change had the intended effect, we should always use a git diff as a first pass for sanity checking. In our experience, having a visual git client like GitHub Desktop or sourcetree is really helpful for this part. In case you prefer the command line: git status git diff 5. Quality control Now it's time to run your quality control checks. This can either happen locally ( 5a ) or through your continuous integration system ( 7/5b ). 5a. Local testing If you chose to run your test locally: sh run.sh make IMP=false test This will run the whole set of configured ODK tests on including your change. If you have a complex DOSDP pattern pipeline you may want to add PAT=false to skip the potentially lengthy process of rebuilding the patterns. sh run.sh make IMP=false PAT=false test 6. Pull request When you are happy with the changes, you commit your changes to your feature branch, push them upstream (to GitHub) and create a pull request. For example: git add NAMEOFCHANGEDFILES git commit -m \"Added biological process term #12\" git push -u origin issue23removeprocess Then you go to your project on GitHub, and create a new pull request from the branch, for example: https://github.com/INCATools/ontology-development-kit/pulls There is a lot of great advise on how to write pull requests, but at the very least you should: - mention the tickets affected: see #23 to link to a related ticket, or fixes #23 if, by merging this pull request, the ticket is fixed. Tickets in the latter case will be closed automatically by GitHub when the pull request is merged. - summarise the changes in a few sentences. Consider the reviewer: what would they want to know right away. - If the diff is large, provide instructions on how to review the pull request best (sometimes, there are many changed files, but only one important change). 7/5b. Continuous Integration Testing If you didn't run and local quality control checks (see 5a ), you should have Continuous Integration (CI) set up, for example: - Travis - GitHub Actions More on how to set this up here . Once the pull request is created, the CI will automatically trigger. If all is fine, it will show up green, otherwise red. 8. Community review Once all the automatic tests have passed, it is important to put a second set of eyes on the pull request. Ontologies are inherently social - as in that they represent some kind of community consensus on how a domain is organised conceptually. This seems high brow talk, but it is very important that as an ontology editor, you have your work validated by the community you are trying to serve (e.g. your colleagues, other contributors etc.). In our experience, it is hard to get more than one review on a pull request - two is great. You can set up GitHub branch protection to actually require a review before a pull request can be merged! We recommend this. This step seems daunting to some hopefully under-resourced ontologies, but we recommend to put this high up on your list of priorities - train a colleague, reach out! 9. Merge and cleanup When the QC is green and the reviews are in (approvals), it is time to merge the pull request. After the pull request is merged, remember to delete the branch as well (this option will show up as a big button right after you have merged the pull request). If you have not done so, close all the associated tickets fixed by the pull request. 10. Changelog (Optional) It is sometimes difficult to keep track of changes made to an ontology. Some ontology teams opt to document changes in a changelog (simply a text file in your repository) so that when release day comes, you know everything you have changed. This is advisable at least for major changes (such as a new release system, a new pattern or template etc.).","title":"Editors Workflow"},{"location":"odk-workflows/EditorsWorkflow/#editors-workflow","text":"The editors workflow is one of the formal workflows to ensure that the ontology is developed correctly according to ontology engineering principles. There are a few different editors workflows: Local editing workflow: Editing the ontology in your local environment by hand, using tools such as Prot\u00e9g\u00e9, ROBOT templates or DOSDP patterns. Completely automated data pipeline (GitHub Actions) DROID workflow This document only covers the first editing workflow, but more will be added in the future","title":"Editors Workflow"},{"location":"odk-workflows/EditorsWorkflow/#local-editing-workflow","text":"Workflow requirements: git github docker editing tool of choice, e.g. Prot\u00e9g\u00e9, your favourite text editor, etc","title":"Local editing workflow"},{"location":"odk-workflows/EditorsWorkflow/#1-create-issue","text":"Ensure that there is a ticket on your issue tracker that describes the change you are about to make. While this seems optional, this is a very important part of the social contract of building an ontology - no change to the ontology should be performed without a good ticket, describing the motivation and nature of the intended change.","title":"1. Create issue"},{"location":"odk-workflows/EditorsWorkflow/#2-update-main-branch","text":"In your local environment (e.g. your laptop), make sure you are on the main (prev. master ) branch and ensure that you have all the upstream changes, for example: git checkout main git pull","title":"2. Update main branch"},{"location":"odk-workflows/EditorsWorkflow/#3-create-feature-branch","text":"Create a new branch. Per convention, we try to use meaningful branch names such as: - issue23removeprocess (where issue 23 is the related issue on GitHub) - issue26addcontributor - release20210101 (for releases) On your command line, this looks like this: git checkout -b issue23removeprocess","title":"3. Create feature branch"},{"location":"odk-workflows/EditorsWorkflow/#4-perform-edit","text":"Using your editor of choice, perform the intended edit. For example: Prot\u00e9g\u00e9 Open src/ontology/omrse-edit.owl in Prot\u00e9g\u00e9 Make the change Save the file TextEdit Open src/ontology/omrse-edit.owl in TextEdit (or Sublime, Atom, Vim, Nano) Make the change Save the file Consider the following when making the edit. According to our development philosophy, the only places that should be manually edited are: src/ontology/omrse-edit.owl Any ROBOT templates you chose to use (the TSV files only) Any DOSDP data tables you chose to use (the TSV files, and potentially the associated patterns) components (anything in src/ontology/components ), see here . Imports should not be edited (any edits will be flushed out with the next update). However, refreshing imports is a potentially breaking change - and is discussed elsewhere . Changes should usually be small. Adding or changing 1 term is great. Adding or changing 10 related terms is ok. Adding or changing 100 or more terms at once should be considered very carefully.","title":"4. Perform edit"},{"location":"odk-workflows/EditorsWorkflow/#4-check-the-git-diff","text":"This step is very important. Rather than simply trusting your change had the intended effect, we should always use a git diff as a first pass for sanity checking. In our experience, having a visual git client like GitHub Desktop or sourcetree is really helpful for this part. In case you prefer the command line: git status git diff","title":"4. Check the Git diff"},{"location":"odk-workflows/EditorsWorkflow/#5-quality-control","text":"Now it's time to run your quality control checks. This can either happen locally ( 5a ) or through your continuous integration system ( 7/5b ).","title":"5. Quality control"},{"location":"odk-workflows/EditorsWorkflow/#5a-local-testing","text":"If you chose to run your test locally: sh run.sh make IMP=false test This will run the whole set of configured ODK tests on including your change. If you have a complex DOSDP pattern pipeline you may want to add PAT=false to skip the potentially lengthy process of rebuilding the patterns. sh run.sh make IMP=false PAT=false test","title":"5a. Local testing"},{"location":"odk-workflows/EditorsWorkflow/#6-pull-request","text":"When you are happy with the changes, you commit your changes to your feature branch, push them upstream (to GitHub) and create a pull request. For example: git add NAMEOFCHANGEDFILES git commit -m \"Added biological process term #12\" git push -u origin issue23removeprocess Then you go to your project on GitHub, and create a new pull request from the branch, for example: https://github.com/INCATools/ontology-development-kit/pulls There is a lot of great advise on how to write pull requests, but at the very least you should: - mention the tickets affected: see #23 to link to a related ticket, or fixes #23 if, by merging this pull request, the ticket is fixed. Tickets in the latter case will be closed automatically by GitHub when the pull request is merged. - summarise the changes in a few sentences. Consider the reviewer: what would they want to know right away. - If the diff is large, provide instructions on how to review the pull request best (sometimes, there are many changed files, but only one important change).","title":"6. Pull request"},{"location":"odk-workflows/EditorsWorkflow/#75b-continuous-integration-testing","text":"If you didn't run and local quality control checks (see 5a ), you should have Continuous Integration (CI) set up, for example: - Travis - GitHub Actions More on how to set this up here . Once the pull request is created, the CI will automatically trigger. If all is fine, it will show up green, otherwise red.","title":"7/5b. Continuous Integration Testing"},{"location":"odk-workflows/EditorsWorkflow/#8-community-review","text":"Once all the automatic tests have passed, it is important to put a second set of eyes on the pull request. Ontologies are inherently social - as in that they represent some kind of community consensus on how a domain is organised conceptually. This seems high brow talk, but it is very important that as an ontology editor, you have your work validated by the community you are trying to serve (e.g. your colleagues, other contributors etc.). In our experience, it is hard to get more than one review on a pull request - two is great. You can set up GitHub branch protection to actually require a review before a pull request can be merged! We recommend this. This step seems daunting to some hopefully under-resourced ontologies, but we recommend to put this high up on your list of priorities - train a colleague, reach out!","title":"8. Community review"},{"location":"odk-workflows/EditorsWorkflow/#9-merge-and-cleanup","text":"When the QC is green and the reviews are in (approvals), it is time to merge the pull request. After the pull request is merged, remember to delete the branch as well (this option will show up as a big button right after you have merged the pull request). If you have not done so, close all the associated tickets fixed by the pull request.","title":"9. Merge and cleanup"},{"location":"odk-workflows/EditorsWorkflow/#10-changelog-optional","text":"It is sometimes difficult to keep track of changes made to an ontology. Some ontology teams opt to document changes in a changelog (simply a text file in your repository) so that when release day comes, you know everything you have changed. This is advisable at least for major changes (such as a new release system, a new pattern or template etc.).","title":"10. Changelog (Optional)"},{"location":"odk-workflows/ManageAutomatedTest/","text":"Constraint violation checks We can define custom checks using SPARQL . SPARQL queries define bad modelling patterns (missing labels, misspelt URIs, and many more) in the ontology. If these queries return any results, then the build will fail. Custom checks are designed to be run as part of GitHub Actions Continuous Integration testing, but they can also run locally. Steps to add a constraint violation check: Add the SPARQL query in src/sparql . The name of the file should end with -violation.sparql . Please give a name that helps to understand which violation the query wants to check. Add the name of the new file to odk configuration file src/ontology/uberon-odk.yaml : Include the name of the file (without the -violation.sparql part) to the list inside the key custom_sparql_checks that is inside robot_report key. If the robot_report or custom_sparql_checks keys are not available, please add this code block to the end of the file. yaml robot_report: release_reports: False fail_on: ERROR use_labels: False custom_profile: True report_on: - edit custom_sparql_checks: - name-of-the-file-check 3. Update the repository so your new SPARQL check will be included in the QC. sh run.sh make update_repo","title":"ManageAutomatedTest"},{"location":"odk-workflows/ManageAutomatedTest/#constraint-violation-checks","text":"We can define custom checks using SPARQL . SPARQL queries define bad modelling patterns (missing labels, misspelt URIs, and many more) in the ontology. If these queries return any results, then the build will fail. Custom checks are designed to be run as part of GitHub Actions Continuous Integration testing, but they can also run locally.","title":"Constraint violation checks"},{"location":"odk-workflows/ManageAutomatedTest/#steps-to-add-a-constraint-violation-check","text":"Add the SPARQL query in src/sparql . The name of the file should end with -violation.sparql . Please give a name that helps to understand which violation the query wants to check. Add the name of the new file to odk configuration file src/ontology/uberon-odk.yaml : Include the name of the file (without the -violation.sparql part) to the list inside the key custom_sparql_checks that is inside robot_report key. If the robot_report or custom_sparql_checks keys are not available, please add this code block to the end of the file. yaml robot_report: release_reports: False fail_on: ERROR use_labels: False custom_profile: True report_on: - edit custom_sparql_checks: - name-of-the-file-check 3. Update the repository so your new SPARQL check will be included in the QC. sh run.sh make update_repo","title":"Steps to add a constraint violation check:"},{"location":"odk-workflows/ManageDocumentation/","text":"Updating the Documentation The documentation for OMRSE is managed in two places (relative to the repository root): The docs directory contains all the files that pertain to the content of the documentation (more below) the mkdocs.yaml file contains the documentation config, in particular its navigation bar and theme. The documentation is hosted using GitHub pages, on a special branch of the repository (called gh-pages ). It is important that this branch is never deleted - it contains all the files GitHub pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually . All changes to the docs happen inside the docs directory on the main branch. Editing the docs Changing content All the documentation is contained in the docs directory, and is managed in Markdown . Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. This is a normal editing workflow: Open the .md file you want to change in an editor of choice (a simple text editor is often best). IMPORTANT : Do not edit any files in the docs/odk-workflows/ directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker . Perform the edit and save the file Commit the file to a branch, and create a pull request as usual. If your development team likes your changes, merge the docs into main branch. Deploy the documentation (see below) Deploy the documentation The documentation is not automatically updated from the Markdown, and needs to be deployed deliberately. To do this, perform the following steps: In your terminal, navigate to the edit directory of your ontology, e.g.: cd omrse/src/ontology Now you are ready to build the docs as follows: sh run.sh make update_docs Mkdocs now sets off to build the site from the markdown pages. You will be asked to Enter your username Enter your password (see here for using GitHub access tokens instead) IMPORTANT : Using password based authentication will be deprecated this year (2021). Make sure you read up on personal access tokens if that happens! If everything was successful, you will see a message similar to this one: INFO - Your documentation should shortly be available at: https://mcwdsi.github.io/OMRSE/ 3. Just to double check, you can now navigate to your documentation pages (usually https://mcwdsi.github.io/OMRSE/). Just make sure you give GitHub 2-5 minutes to build the pages!","title":"Updating the Documentation"},{"location":"odk-workflows/ManageDocumentation/#updating-the-documentation","text":"The documentation for OMRSE is managed in two places (relative to the repository root): The docs directory contains all the files that pertain to the content of the documentation (more below) the mkdocs.yaml file contains the documentation config, in particular its navigation bar and theme. The documentation is hosted using GitHub pages, on a special branch of the repository (called gh-pages ). It is important that this branch is never deleted - it contains all the files GitHub pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually . All changes to the docs happen inside the docs directory on the main branch.","title":"Updating the Documentation"},{"location":"odk-workflows/ManageDocumentation/#editing-the-docs","text":"","title":"Editing the docs"},{"location":"odk-workflows/ManageDocumentation/#changing-content","text":"All the documentation is contained in the docs directory, and is managed in Markdown . Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. This is a normal editing workflow: Open the .md file you want to change in an editor of choice (a simple text editor is often best). IMPORTANT : Do not edit any files in the docs/odk-workflows/ directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker . Perform the edit and save the file Commit the file to a branch, and create a pull request as usual. If your development team likes your changes, merge the docs into main branch. Deploy the documentation (see below)","title":"Changing content"},{"location":"odk-workflows/ManageDocumentation/#deploy-the-documentation","text":"The documentation is not automatically updated from the Markdown, and needs to be deployed deliberately. To do this, perform the following steps: In your terminal, navigate to the edit directory of your ontology, e.g.: cd omrse/src/ontology Now you are ready to build the docs as follows: sh run.sh make update_docs Mkdocs now sets off to build the site from the markdown pages. You will be asked to Enter your username Enter your password (see here for using GitHub access tokens instead) IMPORTANT : Using password based authentication will be deprecated this year (2021). Make sure you read up on personal access tokens if that happens! If everything was successful, you will see a message similar to this one: INFO - Your documentation should shortly be available at: https://mcwdsi.github.io/OMRSE/ 3. Just to double check, you can now navigate to your documentation pages (usually https://mcwdsi.github.io/OMRSE/). Just make sure you give GitHub 2-5 minutes to build the pages!","title":"Deploy the documentation"},{"location":"odk-workflows/ReleaseWorkflow/","text":"The release workflow The release workflow recommended by the ODK is based on GitHub releases and works as follows: Run a release with the ODK Review the release Merge to main branch Create a GitHub release These steps are outlined in detail in the following. Run a release with the ODK Preparation: Ensure that all your pull requests are merged into your main (master) branch Make sure that all changes to main are committed to GitHub ( git status should say that there are no modified files) Locally make sure you have the latest changes from main ( git pull ) Checkout a new branch (e.g. git checkout -b release-2021-01-01 ) You may or may not want to refresh your imports as part of your release strategy (see here ) Make sure you have the latest ODK installed by running docker pull obolibrary/odkfull To actually run the release, you: Open a command line terminal window and navigate to the src/ontology directory ( cd omrse/src/ontology ) Run release pipeline: sh run.sh make prepare_release -B . Note that for some ontologies, this process can take up to 90 minutes - especially if there are large ontologies you depend on, like PRO or CHEBI. If everything went well, you should see the following output on your machine: Release files are now in ../.. - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab . This will create all the specified release targets (OBO, OWL, JSON, and the variants, ont-full and ont-base) and copy them into your release directory (the top level of your repo). Review the release (Optional) Rough check. This step is frequently skipped, but for the more paranoid among us (like the author of this doc), this is a 3 minute additional effort for some peace of mind. Open the main release (omrse.owl) in you favourite development environment (i.e. Prot\u00e9g\u00e9) and eyeball the hierarchy. We recommend two simple checks: Does the very top level of the hierarchy look ok? This means that all new terms have been imported/updated correctly. Does at least one change that you know should be in this release appear? For example, a new class. This means that the release was actually based on the recent edit file. Commit your changes to the branch and make a pull request In your GitHub pull request, review the following three files in detail (based on our experience): omrse.obo - this reflects a useful subset of the whole ontology (everything that can be covered by OBO format). OBO format has that speaking for it: it is very easy to review! omrse-base.owl - this reflects the asserted axioms in your ontology that you have actually edited. Ideally also take a look at omrse-full.owl , which may reveal interesting new inferences you did not know about. Note that the diff of this file is sometimes quite large. Like with every pull request, we recommend to always employ a second set of eyes when reviewing a PR! Merge the main branch Once your CI checks have passed, and your reviews are completed, you can now merge the branch into your main branch (don't forget to delete the branch afterwards - a big button will appear after the merge is finished). Create a GitHub release Go to your releases page on GitHub by navigating to your repository, and then clicking on releases (usually on the right, for example: https://github.com/mcwdsi/OMRSE/releases). Then click \"Draft new release\" As the tag version you need to choose the date on which your ontologies were build. You can find this, for example, by looking at the omrse.obo file and check the data-version: property. The date needs to be prefixed with a v , so, for example v2020-02-06 . You can write whatever you want in the release title, but we typically write the date again. The description underneath should contain a concise list of changes or term additions. Click \"Publish release\". Done. Debugging typical ontology release problems Problems with memory When you are dealing with large ontologies, you need a lot of memory. When you see error messages relating to large ontologies such as CHEBI, PRO, NCBITAXON, or Uberon, you should think of memory first, see here . Problems when using OBO format based tools Sometimes you will get cryptic error messages when using legacy tools using OBO format, such as the ontology release tool (OORT), which is also available as part of the ODK docker container. In these cases, you need to track down what axiom or annotation actually caused the breakdown. In our experience (in about 60% of the cases) the problem lies with duplicate annotations ( def , comment ) which are illegal in OBO. Here is an example recipe of how to deal with such a problem: If you get a message like make: *** [cl.Makefile:84: oort] Error 255 you might have a OORT error. To debug this, in your terminal enter sh run.sh make IMP=false PAT=false oort -B (assuming you are already in the ontology folder in your directory) This should show you where the error is in the log (eg multiple different definitions) WARNING: THE FIX BELOW IS NOT IDEAL, YOU SHOULD ALWAYS TRY TO FIX UPSTREAM IF POSSIBLE Open omrse-edit.owl in Prot\u00e9g\u00e9 and find the offending term and delete all offending issue (e.g. delete ALL definition, if the problem was \"multiple def tags not allowed\") and save. *While this is not idea, as it will remove all definitions from that term, it will be added back again when the term is fixed in the ontology it was imported from and added back in. Rerun sh run.sh make IMP=false PAT=false oort -B and if it all passes, commit your changes to a branch and make a pull request as usual.","title":"Release Workflow"},{"location":"odk-workflows/ReleaseWorkflow/#the-release-workflow","text":"The release workflow recommended by the ODK is based on GitHub releases and works as follows: Run a release with the ODK Review the release Merge to main branch Create a GitHub release These steps are outlined in detail in the following.","title":"The release workflow"},{"location":"odk-workflows/ReleaseWorkflow/#run-a-release-with-the-odk","text":"Preparation: Ensure that all your pull requests are merged into your main (master) branch Make sure that all changes to main are committed to GitHub ( git status should say that there are no modified files) Locally make sure you have the latest changes from main ( git pull ) Checkout a new branch (e.g. git checkout -b release-2021-01-01 ) You may or may not want to refresh your imports as part of your release strategy (see here ) Make sure you have the latest ODK installed by running docker pull obolibrary/odkfull To actually run the release, you: Open a command line terminal window and navigate to the src/ontology directory ( cd omrse/src/ontology ) Run release pipeline: sh run.sh make prepare_release -B . Note that for some ontologies, this process can take up to 90 minutes - especially if there are large ontologies you depend on, like PRO or CHEBI. If everything went well, you should see the following output on your machine: Release files are now in ../.. - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab . This will create all the specified release targets (OBO, OWL, JSON, and the variants, ont-full and ont-base) and copy them into your release directory (the top level of your repo).","title":"Run a release with the ODK"},{"location":"odk-workflows/ReleaseWorkflow/#review-the-release","text":"(Optional) Rough check. This step is frequently skipped, but for the more paranoid among us (like the author of this doc), this is a 3 minute additional effort for some peace of mind. Open the main release (omrse.owl) in you favourite development environment (i.e. Prot\u00e9g\u00e9) and eyeball the hierarchy. We recommend two simple checks: Does the very top level of the hierarchy look ok? This means that all new terms have been imported/updated correctly. Does at least one change that you know should be in this release appear? For example, a new class. This means that the release was actually based on the recent edit file. Commit your changes to the branch and make a pull request In your GitHub pull request, review the following three files in detail (based on our experience): omrse.obo - this reflects a useful subset of the whole ontology (everything that can be covered by OBO format). OBO format has that speaking for it: it is very easy to review! omrse-base.owl - this reflects the asserted axioms in your ontology that you have actually edited. Ideally also take a look at omrse-full.owl , which may reveal interesting new inferences you did not know about. Note that the diff of this file is sometimes quite large. Like with every pull request, we recommend to always employ a second set of eyes when reviewing a PR!","title":"Review the release"},{"location":"odk-workflows/ReleaseWorkflow/#merge-the-main-branch","text":"Once your CI checks have passed, and your reviews are completed, you can now merge the branch into your main branch (don't forget to delete the branch afterwards - a big button will appear after the merge is finished).","title":"Merge the main branch"},{"location":"odk-workflows/ReleaseWorkflow/#create-a-github-release","text":"Go to your releases page on GitHub by navigating to your repository, and then clicking on releases (usually on the right, for example: https://github.com/mcwdsi/OMRSE/releases). Then click \"Draft new release\" As the tag version you need to choose the date on which your ontologies were build. You can find this, for example, by looking at the omrse.obo file and check the data-version: property. The date needs to be prefixed with a v , so, for example v2020-02-06 . You can write whatever you want in the release title, but we typically write the date again. The description underneath should contain a concise list of changes or term additions. Click \"Publish release\". Done.","title":"Create a GitHub release"},{"location":"odk-workflows/ReleaseWorkflow/#debugging-typical-ontology-release-problems","text":"","title":"Debugging typical ontology release problems"},{"location":"odk-workflows/ReleaseWorkflow/#problems-with-memory","text":"When you are dealing with large ontologies, you need a lot of memory. When you see error messages relating to large ontologies such as CHEBI, PRO, NCBITAXON, or Uberon, you should think of memory first, see here .","title":"Problems with memory"},{"location":"odk-workflows/ReleaseWorkflow/#problems-when-using-obo-format-based-tools","text":"Sometimes you will get cryptic error messages when using legacy tools using OBO format, such as the ontology release tool (OORT), which is also available as part of the ODK docker container. In these cases, you need to track down what axiom or annotation actually caused the breakdown. In our experience (in about 60% of the cases) the problem lies with duplicate annotations ( def , comment ) which are illegal in OBO. Here is an example recipe of how to deal with such a problem: If you get a message like make: *** [cl.Makefile:84: oort] Error 255 you might have a OORT error. To debug this, in your terminal enter sh run.sh make IMP=false PAT=false oort -B (assuming you are already in the ontology folder in your directory) This should show you where the error is in the log (eg multiple different definitions) WARNING: THE FIX BELOW IS NOT IDEAL, YOU SHOULD ALWAYS TRY TO FIX UPSTREAM IF POSSIBLE Open omrse-edit.owl in Prot\u00e9g\u00e9 and find the offending term and delete all offending issue (e.g. delete ALL definition, if the problem was \"multiple def tags not allowed\") and save. *While this is not idea, as it will remove all definitions from that term, it will be added back again when the term is fixed in the ontology it was imported from and added back in. Rerun sh run.sh make IMP=false PAT=false oort -B and if it all passes, commit your changes to a branch and make a pull request as usual.","title":"Problems when using OBO format based tools"},{"location":"odk-workflows/RepoManagement/","text":"Managing your ODK repository Updating your ODK repository Your ODK repositories configuration is managed in src/ontology/omrse-odk.yaml . The ODK Project Configuration Schema defines all possible parameters that can be used in this config YAML. Once you have made your changes, you can run the following to apply your changes to the repository: sh run.sh make update_repo There are a large number of options that can be set to configure your ODK, but we will only discuss a few of them here. NOTE for Windows users: You may get a cryptic failure such as Set Illegal Option - if the update script located in src/scripts/update_repo.sh was saved using Windows Line endings. These need to change to unix line endings. In Notepad++, for example, you can click on Edit->EOL Conversion->Unix LF to change this. Managing imports You can use the update repository workflow described on this page to perform the following operations to your imports: Add a new import Modify an existing import Remove an import you no longer want Customise an import We will discuss all these workflows in the following. Add new import To add a new import, you first edit your odk config as described above , adding an id to the product list in the import_group section (for the sake of this example, we assume you already import RO, and your goal is to also import GO): import_group: products: - id: ro - id: go Note: our ODK file should only have one import_group which can contain multiple imports (in the products section). Next, you run the update repo workflow to apply these changes. Note that by default, this module is going to be a SLME Bottom module, see here . To change that or customise your module, see section \"Customise an import\". To finalise the addition of your import, perform the following steps: Add an import statement to your src/ontology/omrse-edit.owl file. We suggest to do this using a text editor, by simply copying an existing import declaration and renaming it to the new ontology import, for example as follows: ... Ontology(<http://purl.obolibrary.org/obo/omrse.owl> Import(<http://purl.obolibrary.org/obo/omrse/imports/ro_import.owl>) Import(<http://purl.obolibrary.org/obo/omrse/imports/go_import.owl>) ... Add your imports redirect to your catalog file src/ontology/catalog-v001.xml , for example: <uri name=\"http://purl.obolibrary.org/obo/omrse/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> Test whether everything is in order: Refresh your import Open in your Ontology Editor of choice (Protege) and ensure that the expected terms are imported. Note: The catalog file src/ontology/catalog-v001.xml has one purpose: redirecting imports from URLs to local files. For example, if you have Import(<http://purl.obolibrary.org/obo/omrse/imports/go_import.owl>) in your editors file (the ontology) and <uri name=\"http://purl.obolibrary.org/obo/omrse/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> in your catalog, tools like robot or Prot\u00e9g\u00e9 will recognize the statement in the catalog file to redirect the URL http://purl.obolibrary.org/obo/omrse/imports/go_import.owl to the local file imports/go_import.owl (which is in your src/ontology directory). Modify an existing import If you simply wish to refresh your import in light of new terms, see here . If you wish to change the type of your module see section \"Customise an import\". Remove an existing import To remove an existing import, perform the following steps: remove the import declaration from your src/ontology/omrse-edit.owl . remove the id from your src/ontology/omrse-odk.yaml , eg. - id: go from the list of products in the import_group . run update repo workflow delete the associated files manually: src/imports/go_import.owl src/imports/go_terms.txt Remove the respective entry from the src/ontology/catalog-v001.xml file. Customise an import By default, an import module extracted from a source ontology will be a SLME module, see here . There are various options to change the default. The following change to your repo config ( src/ontology/omrse-odk.yaml ) will switch the go import from an SLME module to a simple ROBOT filter module: import_group: products: - id: ro - id: go module_type: filter A ROBOT filter module is, essentially, importing all external terms declared by your ontology (see here on how to declare external terms to be imported). Note that the filter module does not consider terms/annotations from namespaces other than the base-namespace of the ontology itself. For example, in the example of GO above, only annotations / axioms related to the GO base IRI (http://purl.obolibrary.org/obo/GO_) would be considered. This behaviour can be changed by adding additional base IRIs as follows: import_group: products: - id: go module_type: filter base_iris: - http://purl.obolibrary.org/obo/GO_ - http://purl.obolibrary.org/obo/CL_ - http://purl.obolibrary.org/obo/BFO If you wish to customise your import entirely, you can specify your own ROBOT command to do so. To do that, add the following to your repo config ( src/ontology/omrse-odk.yaml ): import_group: products: - id: ro - id: go module_type: custom Now add a new goal in your custom Makefile ( src/ontology/omrse.Makefile , not src/ontology/Makefile ). imports/go_import.owl: mirror/ro.owl imports/ro_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \\ extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi Now feel free to change this goal to do whatever you wish it to do! It probably makes some sense (albeit not being a strict necessity), to leave most of the goal instead and replace only: extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ to another ROBOT pipeline. Add a component A component is an import which belongs to your ontology, e.g. is managed by you and your team. Open src/ontology/omrse-odk.yaml If you dont have it yet, add a new top level section components Under the components section, add a new section called products . This is where all your components are specified Under the products section, add a new component, e.g. - filename: mycomp.owl Example components: products: - filename: mycomp.owl When running sh run.sh make update_repo , a new file src/ontology/components/mycomp.owl will be created which you can edit as you see fit. Typical ways to edit: Using a ROBOT template to generate the component (see below) Manually curating the component separately with Prot\u00e9g\u00e9 or any other editor Providing a components/mycomp.owl: make target in src/ontology/omrse.Makefile and provide a custom command to generate the component WARNING : Note that the custom rule to generate the component MUST NOT depend on any other ODK-generated file such as seed files and the like (see issue ). Providing an additional attribute for the component in src/ontology/omrse-odk.yaml , source , to specify that this component should simply be downloaded from somewhere on the web. Adding a new component based on a ROBOT template Since ODK 1.3.2, it is possible to simply link a ROBOT template to a component without having to specify any of the import logic. In order to add a new component that is connected to one or more template files, follow these steps: Open src/ontology/omrse-odk.yaml . Make sure that use_templates: TRUE is set in the global project options. You should also make sure that use_context: TRUE is set in case you are using prefixes in your templates that are not known to robot , such as OMOP: , CPONT: and more. All non-standard prefixes you are using should be added to config/context.json . Add another component to the products section. To activate this component to be template-driven, simply say: use_template: TRUE . This will create an empty template for you in the templates directory, which will automatically be processed when recreating the component (e.g. run.bat make recreate-mycomp ). If you want to use more than one component, use the templates field to add as many template names as you wish. ODK will look for them in the src/templates directory. Advanced: If you want to provide additional processing options, you can use the template_options field. This should be a string with option from robot template . One typical example for additional options you may want to provide is --add-prefixes config/context.json to ensure the prefix map of your context is provided to robot , see above. Example : components: products: - filename: mycomp.owl use_template: TRUE template_options: --add-prefixes config/context.json templates: - template1.tsv - template2.tsv Note : if your mirror is particularly large and complex, read this ODK recommendation .","title":"Manage your ODK Repository"},{"location":"odk-workflows/RepoManagement/#managing-your-odk-repository","text":"","title":"Managing your ODK repository"},{"location":"odk-workflows/RepoManagement/#updating-your-odk-repository","text":"Your ODK repositories configuration is managed in src/ontology/omrse-odk.yaml . The ODK Project Configuration Schema defines all possible parameters that can be used in this config YAML. Once you have made your changes, you can run the following to apply your changes to the repository: sh run.sh make update_repo There are a large number of options that can be set to configure your ODK, but we will only discuss a few of them here. NOTE for Windows users: You may get a cryptic failure such as Set Illegal Option - if the update script located in src/scripts/update_repo.sh was saved using Windows Line endings. These need to change to unix line endings. In Notepad++, for example, you can click on Edit->EOL Conversion->Unix LF to change this.","title":"Updating your ODK repository"},{"location":"odk-workflows/RepoManagement/#managing-imports","text":"You can use the update repository workflow described on this page to perform the following operations to your imports: Add a new import Modify an existing import Remove an import you no longer want Customise an import We will discuss all these workflows in the following.","title":"Managing imports"},{"location":"odk-workflows/RepoManagement/#add-new-import","text":"To add a new import, you first edit your odk config as described above , adding an id to the product list in the import_group section (for the sake of this example, we assume you already import RO, and your goal is to also import GO): import_group: products: - id: ro - id: go Note: our ODK file should only have one import_group which can contain multiple imports (in the products section). Next, you run the update repo workflow to apply these changes. Note that by default, this module is going to be a SLME Bottom module, see here . To change that or customise your module, see section \"Customise an import\". To finalise the addition of your import, perform the following steps: Add an import statement to your src/ontology/omrse-edit.owl file. We suggest to do this using a text editor, by simply copying an existing import declaration and renaming it to the new ontology import, for example as follows: ... Ontology(<http://purl.obolibrary.org/obo/omrse.owl> Import(<http://purl.obolibrary.org/obo/omrse/imports/ro_import.owl>) Import(<http://purl.obolibrary.org/obo/omrse/imports/go_import.owl>) ... Add your imports redirect to your catalog file src/ontology/catalog-v001.xml , for example: <uri name=\"http://purl.obolibrary.org/obo/omrse/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> Test whether everything is in order: Refresh your import Open in your Ontology Editor of choice (Protege) and ensure that the expected terms are imported. Note: The catalog file src/ontology/catalog-v001.xml has one purpose: redirecting imports from URLs to local files. For example, if you have Import(<http://purl.obolibrary.org/obo/omrse/imports/go_import.owl>) in your editors file (the ontology) and <uri name=\"http://purl.obolibrary.org/obo/omrse/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> in your catalog, tools like robot or Prot\u00e9g\u00e9 will recognize the statement in the catalog file to redirect the URL http://purl.obolibrary.org/obo/omrse/imports/go_import.owl to the local file imports/go_import.owl (which is in your src/ontology directory).","title":"Add new import"},{"location":"odk-workflows/RepoManagement/#modify-an-existing-import","text":"If you simply wish to refresh your import in light of new terms, see here . If you wish to change the type of your module see section \"Customise an import\".","title":"Modify an existing import"},{"location":"odk-workflows/RepoManagement/#remove-an-existing-import","text":"To remove an existing import, perform the following steps: remove the import declaration from your src/ontology/omrse-edit.owl . remove the id from your src/ontology/omrse-odk.yaml , eg. - id: go from the list of products in the import_group . run update repo workflow delete the associated files manually: src/imports/go_import.owl src/imports/go_terms.txt Remove the respective entry from the src/ontology/catalog-v001.xml file.","title":"Remove an existing import"},{"location":"odk-workflows/RepoManagement/#customise-an-import","text":"By default, an import module extracted from a source ontology will be a SLME module, see here . There are various options to change the default. The following change to your repo config ( src/ontology/omrse-odk.yaml ) will switch the go import from an SLME module to a simple ROBOT filter module: import_group: products: - id: ro - id: go module_type: filter A ROBOT filter module is, essentially, importing all external terms declared by your ontology (see here on how to declare external terms to be imported). Note that the filter module does not consider terms/annotations from namespaces other than the base-namespace of the ontology itself. For example, in the example of GO above, only annotations / axioms related to the GO base IRI (http://purl.obolibrary.org/obo/GO_) would be considered. This behaviour can be changed by adding additional base IRIs as follows: import_group: products: - id: go module_type: filter base_iris: - http://purl.obolibrary.org/obo/GO_ - http://purl.obolibrary.org/obo/CL_ - http://purl.obolibrary.org/obo/BFO If you wish to customise your import entirely, you can specify your own ROBOT command to do so. To do that, add the following to your repo config ( src/ontology/omrse-odk.yaml ): import_group: products: - id: ro - id: go module_type: custom Now add a new goal in your custom Makefile ( src/ontology/omrse.Makefile , not src/ontology/Makefile ). imports/go_import.owl: mirror/ro.owl imports/ro_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \\ extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi Now feel free to change this goal to do whatever you wish it to do! It probably makes some sense (albeit not being a strict necessity), to leave most of the goal instead and replace only: extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ to another ROBOT pipeline.","title":"Customise an import"},{"location":"odk-workflows/RepoManagement/#add-a-component","text":"A component is an import which belongs to your ontology, e.g. is managed by you and your team. Open src/ontology/omrse-odk.yaml If you dont have it yet, add a new top level section components Under the components section, add a new section called products . This is where all your components are specified Under the products section, add a new component, e.g. - filename: mycomp.owl Example components: products: - filename: mycomp.owl When running sh run.sh make update_repo , a new file src/ontology/components/mycomp.owl will be created which you can edit as you see fit. Typical ways to edit: Using a ROBOT template to generate the component (see below) Manually curating the component separately with Prot\u00e9g\u00e9 or any other editor Providing a components/mycomp.owl: make target in src/ontology/omrse.Makefile and provide a custom command to generate the component WARNING : Note that the custom rule to generate the component MUST NOT depend on any other ODK-generated file such as seed files and the like (see issue ). Providing an additional attribute for the component in src/ontology/omrse-odk.yaml , source , to specify that this component should simply be downloaded from somewhere on the web.","title":"Add a component"},{"location":"odk-workflows/RepoManagement/#adding-a-new-component-based-on-a-robot-template","text":"Since ODK 1.3.2, it is possible to simply link a ROBOT template to a component without having to specify any of the import logic. In order to add a new component that is connected to one or more template files, follow these steps: Open src/ontology/omrse-odk.yaml . Make sure that use_templates: TRUE is set in the global project options. You should also make sure that use_context: TRUE is set in case you are using prefixes in your templates that are not known to robot , such as OMOP: , CPONT: and more. All non-standard prefixes you are using should be added to config/context.json . Add another component to the products section. To activate this component to be template-driven, simply say: use_template: TRUE . This will create an empty template for you in the templates directory, which will automatically be processed when recreating the component (e.g. run.bat make recreate-mycomp ). If you want to use more than one component, use the templates field to add as many template names as you wish. ODK will look for them in the src/templates directory. Advanced: If you want to provide additional processing options, you can use the template_options field. This should be a string with option from robot template . One typical example for additional options you may want to provide is --add-prefixes config/context.json to ensure the prefix map of your context is provided to robot , see above. Example : components: products: - filename: mycomp.owl use_template: TRUE template_options: --add-prefixes config/context.json templates: - template1.tsv - template2.tsv Note : if your mirror is particularly large and complex, read this ODK recommendation .","title":"Adding a new component based on a ROBOT template"},{"location":"odk-workflows/RepositoryFileStructure/","text":"Repository structure The main kinds of files in the repository: Release files Imports Components Release files Release file are the file that are considered part of the official ontology release and to be used by the community. A detailed description of the release artefacts can be found here . Imports Imports are subsets of external ontologies that contain terms and axioms you would like to re-use in your ontology. These are considered \"external\", like dependencies in software development, and are not included in your \"base\" product, which is the release artefact which contains only those axioms that you personally maintain. These are the current imports in OMRSE Import URL Type ro http://purl.obolibrary.org/obo/ro.owl slme ogms http://purl.obolibrary.org/obo/ogms.owl filter obi http://purl.obolibrary.org/obo/obi.owl filter apollo_sv http://purl.obolibrary.org/obo/apollo_sv.owl filter obib http://purl.obolibrary.org/obo/obib.owl filter go http://purl.obolibrary.org/obo/go.owl filter oae http://purl.obolibrary.org/obo/oae.owl filter pco http://purl.obolibrary.org/obo/pco.owl filter bfo http://purl.obolibrary.org/obo/bfo.owl slme ncbitaxon http://purl.obolibrary.org/obo/ncbitaxon/subsets/taxslim.owl filter oostt http://purl.obolibrary.org/obo/oostt.owl filter omo http://purl.obolibrary.org/obo/omo.owl mirror iao http://purl.obolibrary.org/obo/iao.owl mirror d-acts http://purl.obolibrary.org/obo/iao/d-acts.owl filter geo http://purl.obolibrary.org/obo/geo.owl filter mf http://purl.obolibrary.org/obo/mf.owl filter occo http://purl.obolibrary.org/obo/occo.owl filter ## Components Components, in contrast to imports, are considered full members of the ontology. This means that any axiom in a component is also included in the ontology base - which means it is considered native to the ontology. While this sounds complicated, consider this: conceptually, no component should be part of more than one ontology. If that seems to be the case, we are most likely talking about an import. Components are often not needed for ontologies, but there are some use cases: There is an automated process that generates and re-generates a part of the ontology A part of the ontology is managed in ROBOT templates The expressivity of the component is higher than the format of the edit file. For example, people still choose to manage their ontology in OBO format (they should not) missing out on a lot of owl features. They may choose to manage logic that is beyond OBO in a specific OWL component. These are the components in OMRSE Filename URL language/language-individuals.owl None","title":"Your ODK Repository Overview"},{"location":"odk-workflows/RepositoryFileStructure/#repository-structure","text":"The main kinds of files in the repository: Release files Imports Components","title":"Repository structure"},{"location":"odk-workflows/RepositoryFileStructure/#release-files","text":"Release file are the file that are considered part of the official ontology release and to be used by the community. A detailed description of the release artefacts can be found here .","title":"Release files"},{"location":"odk-workflows/RepositoryFileStructure/#imports","text":"Imports are subsets of external ontologies that contain terms and axioms you would like to re-use in your ontology. These are considered \"external\", like dependencies in software development, and are not included in your \"base\" product, which is the release artefact which contains only those axioms that you personally maintain. These are the current imports in OMRSE Import URL Type ro http://purl.obolibrary.org/obo/ro.owl slme ogms http://purl.obolibrary.org/obo/ogms.owl filter obi http://purl.obolibrary.org/obo/obi.owl filter apollo_sv http://purl.obolibrary.org/obo/apollo_sv.owl filter obib http://purl.obolibrary.org/obo/obib.owl filter go http://purl.obolibrary.org/obo/go.owl filter oae http://purl.obolibrary.org/obo/oae.owl filter pco http://purl.obolibrary.org/obo/pco.owl filter bfo http://purl.obolibrary.org/obo/bfo.owl slme ncbitaxon http://purl.obolibrary.org/obo/ncbitaxon/subsets/taxslim.owl filter oostt http://purl.obolibrary.org/obo/oostt.owl filter omo http://purl.obolibrary.org/obo/omo.owl mirror iao http://purl.obolibrary.org/obo/iao.owl mirror d-acts http://purl.obolibrary.org/obo/iao/d-acts.owl filter geo http://purl.obolibrary.org/obo/geo.owl filter mf http://purl.obolibrary.org/obo/mf.owl filter occo http://purl.obolibrary.org/obo/occo.owl filter ## Components Components, in contrast to imports, are considered full members of the ontology. This means that any axiom in a component is also included in the ontology base - which means it is considered native to the ontology. While this sounds complicated, consider this: conceptually, no component should be part of more than one ontology. If that seems to be the case, we are most likely talking about an import. Components are often not needed for ontologies, but there are some use cases: There is an automated process that generates and re-generates a part of the ontology A part of the ontology is managed in ROBOT templates The expressivity of the component is higher than the format of the edit file. For example, people still choose to manage their ontology in OBO format (they should not) missing out on a lot of owl features. They may choose to manage logic that is beyond OBO in a specific OWL component. These are the components in OMRSE Filename URL language/language-individuals.owl None","title":"Imports"},{"location":"odk-workflows/SettingUpDockerForODK/","text":"Setting up your Docker environment for ODK use One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . There are two places you need to consider to set your memory: Your src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/omrse-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Setting up Docker for ODK"},{"location":"odk-workflows/SettingUpDockerForODK/#setting-up-your-docker-environment-for-odk-use","text":"One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . There are two places you need to consider to set your memory: Your src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/omrse-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Setting up your Docker environment for ODK use"},{"location":"odk-workflows/UpdateImports/","text":"Update Imports Workflow This page discusses how to update the contents of your imports, like adding or removing terms. If you are looking to customise imports, like changing the module type, see here . Importing a new term Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\". Importing a new term is split into two sub-phases: Declaring the terms to be imported Refreshing imports dynamically Declaring terms to be imported There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be): Prot\u00e9g\u00e9-based declaration Using term files Using the custom import template Prot\u00e9g\u00e9-based declaration This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container . This approach also applies to ontologies that use base module import approach. Open your ontology (edit file) in Prot\u00e9g\u00e9 (5.5+). Select 'owl:Thing' Add a new class as usual. Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906. Click 'OK' Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here ), the metadata (labels, definitions, etc.) for this term are imported from the respective external source ontology and becomes visible in your ontology. Using term files Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in src/ontology/go_import.owl , you will also have an associated term file src/ontology/go_terms.txt . You can add terms in there simply as a list: GO:0008150 GO:0008151 Now you can run the refresh imports workflow ) and the two terms will be imported. Using the custom import template This workflow is appropriate if: You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above). You wish to augment your imported ontologies with additional information. This requires a cautionary discussion. To enable this workflow, you add the following to your ODK config file ( src/ontology/omrse-odk.yaml ), and update the repository : use_custom_import_module: TRUE Now you can manage your imported terms directly in the custom external terms template, which is located at src/templates/external_import.owl . Note that this file is a ROBOT template , and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully. The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say currently import APOLLO_SV:00000480 , and you wish to import APOLLO_SV:00000532 , you simply add a row like this: ID Entity Type ID TYPE APOLLO_SV:00000480 owl:Class APOLLO_SV:00000532 owl:Class When the imports are refreshed see imports refresh workflow , the term(s) will simply be imported from the configured ontologies. Now, if you wish to extend the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the ID and ENTITY columns and (b) ensure that the ROBOT template is valid otherwise, see here . WARNING . Note that doing this is a widespread antipattern (see related issue ). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file . Refresh imports If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example): First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed . sh run.sh make PAT=false imports/go_import.owl -B Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above: sh run.sh make refresh-go Note that in case you changed the defaults, you need to add IMP=true and/or MIR=true to the command below: sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. go.owl for your go import) you can set MIR=false instead, which will do the exact same thing as the above, but is easier to remember: sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B Using the Base Module approach Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it . A base file is a subset of the ontology that only contains those axioms that nominally belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this: Imagine this being the full Uberon ontology: Axiom 1: BFO:123 SubClassOf BFO:124 Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 The base file is the set of all axioms that are about UBERON terms: Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 I.e. Axiom 1: BFO:123 SubClassOf BFO:124 Gets removed. The base file pipeline is a bit more complex than the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first merging all mirrors into one huge file and then extracting one mega module from it. Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we 1) First obtain the base (usually by simply downloading it, but there is also an option now to create it with ROBOT) 2) We merge all base files into one big pile 3) Then we extract a single module imports/merged_import.owl The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml. To check if your ontology uses this method, check src/ontology/omrse-odk.yaml to see if use_base_merging: TRUE is declared under import_group If your ontology uses Base Module approach, please use the following steps: First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you) Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then refresh imports by running sh run.sh make imports/merged_import.owl Note: if your mirrors are updated, you can run sh run.sh make no-mirror-refresh-merged This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and one of the local devs should pick this up and run the import for you. Lastly, restart Prot\u00e9g\u00e9, and the term should be imported in ready to be used.","title":"Imports management"},{"location":"odk-workflows/UpdateImports/#update-imports-workflow","text":"This page discusses how to update the contents of your imports, like adding or removing terms. If you are looking to customise imports, like changing the module type, see here .","title":"Update Imports Workflow"},{"location":"odk-workflows/UpdateImports/#importing-a-new-term","text":"Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\". Importing a new term is split into two sub-phases: Declaring the terms to be imported Refreshing imports dynamically","title":"Importing a new term"},{"location":"odk-workflows/UpdateImports/#declaring-terms-to-be-imported","text":"There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be): Prot\u00e9g\u00e9-based declaration Using term files Using the custom import template","title":"Declaring terms to be imported"},{"location":"odk-workflows/UpdateImports/#protege-based-declaration","text":"This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container . This approach also applies to ontologies that use base module import approach. Open your ontology (edit file) in Prot\u00e9g\u00e9 (5.5+). Select 'owl:Thing' Add a new class as usual. Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906. Click 'OK' Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here ), the metadata (labels, definitions, etc.) for this term are imported from the respective external source ontology and becomes visible in your ontology.","title":"Prot\u00e9g\u00e9-based declaration"},{"location":"odk-workflows/UpdateImports/#using-term-files","text":"Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in src/ontology/go_import.owl , you will also have an associated term file src/ontology/go_terms.txt . You can add terms in there simply as a list: GO:0008150 GO:0008151 Now you can run the refresh imports workflow ) and the two terms will be imported.","title":"Using term files"},{"location":"odk-workflows/UpdateImports/#using-the-custom-import-template","text":"This workflow is appropriate if: You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above). You wish to augment your imported ontologies with additional information. This requires a cautionary discussion. To enable this workflow, you add the following to your ODK config file ( src/ontology/omrse-odk.yaml ), and update the repository : use_custom_import_module: TRUE Now you can manage your imported terms directly in the custom external terms template, which is located at src/templates/external_import.owl . Note that this file is a ROBOT template , and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully. The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say currently import APOLLO_SV:00000480 , and you wish to import APOLLO_SV:00000532 , you simply add a row like this: ID Entity Type ID TYPE APOLLO_SV:00000480 owl:Class APOLLO_SV:00000532 owl:Class When the imports are refreshed see imports refresh workflow , the term(s) will simply be imported from the configured ontologies. Now, if you wish to extend the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the ID and ENTITY columns and (b) ensure that the ROBOT template is valid otherwise, see here . WARNING . Note that doing this is a widespread antipattern (see related issue ). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file .","title":"Using the custom import template"},{"location":"odk-workflows/UpdateImports/#refresh-imports","text":"If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example): First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed . sh run.sh make PAT=false imports/go_import.owl -B Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above: sh run.sh make refresh-go Note that in case you changed the defaults, you need to add IMP=true and/or MIR=true to the command below: sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. go.owl for your go import) you can set MIR=false instead, which will do the exact same thing as the above, but is easier to remember: sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B","title":"Refresh imports"},{"location":"odk-workflows/UpdateImports/#using-the-base-module-approach","text":"Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it . A base file is a subset of the ontology that only contains those axioms that nominally belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this: Imagine this being the full Uberon ontology: Axiom 1: BFO:123 SubClassOf BFO:124 Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 The base file is the set of all axioms that are about UBERON terms: Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 I.e. Axiom 1: BFO:123 SubClassOf BFO:124 Gets removed. The base file pipeline is a bit more complex than the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first merging all mirrors into one huge file and then extracting one mega module from it. Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we 1) First obtain the base (usually by simply downloading it, but there is also an option now to create it with ROBOT) 2) We merge all base files into one big pile 3) Then we extract a single module imports/merged_import.owl The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml. To check if your ontology uses this method, check src/ontology/omrse-odk.yaml to see if use_base_merging: TRUE is declared under import_group If your ontology uses Base Module approach, please use the following steps: First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you) Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then refresh imports by running sh run.sh make imports/merged_import.owl Note: if your mirrors are updated, you can run sh run.sh make no-mirror-refresh-merged This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and one of the local devs should pick this up and run the import for you. Lastly, restart Prot\u00e9g\u00e9, and the term should be imported in ready to be used.","title":"Using the Base Module approach"},{"location":"odk-workflows/components/","text":"Adding components to an ODK repo For details on what components are, please see component section of repository file structure document . To add custom components to an ODK repo, please follow the following steps: 1) Locate your odk yaml file and open it with your favourite text editor (src/ontology/omrse-odk.yaml) 2) Search if there is already a component section to the yaml file, if not add it accordingly, adding the name of your component: components: products: - filename: your-component-name.owl 3) Refresh your repo by running sh run update_repo . This will automatically (1) create a new file in src/ontology/components/ , (2) update the -edit file so that it imports http://purl.obolibrary.org/obo/omrse/components/your-component-name.owl (the IRI of your new component), and (3) update the XML catalog file ( src/ontology/catalog-v001.xml ) to redirect that IRI to the file in the src/ontology/components directory, so that the new component can be found by tools such as Prot\u00e9g\u00e9 or ROBOT, when they load the -edit file. If your component is to be generated by some automated process, add a goal in your custom Makefile ( src/ontology/omrse.Makefile ) and make it perform any task needed to generate the component: $(COMPONENTSDIR)/your-component-name.owl: $(SRC) <Insert here the code to produce the component> If the component is to be generated from a ROBOT template, the ODK can generate the appropriate code for you. For that, when adding the component fo the ODK configuration file (step 2 above), explicitly indicate that the component should be derived from template(s) and list the source templates: components: products: - filename: your-component-name.owl use_template: true templates: - template1.tsv - template2.tsv In this example, the component will be derived from the templates found in src/templates/template1.tsv and src/templates/template2.tsv . Initial empty templates will automatically be generated when the repository is refreshed (step 3). Likewise, the ODK can generate the required code for the case where the component is to be derived from SSSOM mappings: components: products: - filename: your-component-name.owl use_mappings: true mappings: - my-mappings.sssom.tsv and for the case where the component is to be fetched from a remote resource: components: products: - filename: your-component-name.owl source: https://example.org/component-source.owl","title":"Adding components to an ODK repo"},{"location":"odk-workflows/components/#adding-components-to-an-odk-repo","text":"For details on what components are, please see component section of repository file structure document . To add custom components to an ODK repo, please follow the following steps: 1) Locate your odk yaml file and open it with your favourite text editor (src/ontology/omrse-odk.yaml) 2) Search if there is already a component section to the yaml file, if not add it accordingly, adding the name of your component: components: products: - filename: your-component-name.owl 3) Refresh your repo by running sh run update_repo . This will automatically (1) create a new file in src/ontology/components/ , (2) update the -edit file so that it imports http://purl.obolibrary.org/obo/omrse/components/your-component-name.owl (the IRI of your new component), and (3) update the XML catalog file ( src/ontology/catalog-v001.xml ) to redirect that IRI to the file in the src/ontology/components directory, so that the new component can be found by tools such as Prot\u00e9g\u00e9 or ROBOT, when they load the -edit file. If your component is to be generated by some automated process, add a goal in your custom Makefile ( src/ontology/omrse.Makefile ) and make it perform any task needed to generate the component: $(COMPONENTSDIR)/your-component-name.owl: $(SRC) <Insert here the code to produce the component> If the component is to be generated from a ROBOT template, the ODK can generate the appropriate code for you. For that, when adding the component fo the ODK configuration file (step 2 above), explicitly indicate that the component should be derived from template(s) and list the source templates: components: products: - filename: your-component-name.owl use_template: true templates: - template1.tsv - template2.tsv In this example, the component will be derived from the templates found in src/templates/template1.tsv and src/templates/template2.tsv . Initial empty templates will automatically be generated when the repository is refreshed (step 3). Likewise, the ODK can generate the required code for the case where the component is to be derived from SSSOM mappings: components: products: - filename: your-component-name.owl use_mappings: true mappings: - my-mappings.sssom.tsv and for the case where the component is to be fetched from a remote resource: components: products: - filename: your-component-name.owl source: https://example.org/component-source.owl","title":"Adding components to an ODK repo"},{"location":"omrse-doc-process/EditingOMRSEDocumentation/","text":"Updating OMRSE Documentation NOTE: These instructions supersede the standard Managing Documentation workflow as specified in the ODK, except where explicitly mentioned below. You should follow only these instructions to create, edit, manage, curate, and maintain OMRSE documentation here on the GitHub repo. The documentation for OMRSE is managed in two places (relative to the repository root): The docs directory contains all the files that pertain to the content of the documentation (more below) The mkdocs.yaml file contains the documentation config, in particular its navigation bar and theme. The documentation is hosted using GitHub Pages, on a special branch of the repository (called gh-pages ). It is important that this branch is never deleted - it contains all the files GitHub Pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually . All changes to the docs happen inside the docs directory on the main branch. Editing the docs Changing content All the documentation is contained in the docs directory, and is managed in Markdown . Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. There are two alternative workflows. The first workflow is preferred for more extensive editing including the creation of new documents and folders. The second workflow is more appropriate for minor edits to existing documents. All edits MUST follow one of the two workflows. Important: 1. ALL CHANGES MUST BE REVIEWED BEFORE BEING MERGED INTO THE main BRANCH! 2. Documentation of OMRSE classes, their definitions, other annotations, axioms, and so on should be done on Markdown pages in the modeling folder. Workflow One Create a new branch either locally or on the GitHub site, depending on whether you will create and edit documents on the GitHub site (which is easier) vs. on your local machine. On the new branch, make your additions, changes, edits, etc. If you are working on the GitHub site, commit your changes to the new branch. If you are working locally, once your edits are completed, commit and push them (as well as the new branch if necessary) to the GitHub repo. Create a pull request from the new branch. Request a review of your changes. Workflow Two Create a new .md file or open the existing .md file you want to change on the GitHub site. Once you have completed work on that file, click the green \"Commit changes...\" button. A dialog box will appear. You MUST click the \"Create a new branch for this commit and start a pull request\" radio button. Then write an informative commit message in the \"Extended description\" box and click \"Commit changes\". Create a pull request from the new branch. Request a review of your changes. Deploying the docs Follow the process described in the standard ODK instructions for Updating the Documentation . However, deployment of docs should occur only from the main branch, after the changes have been reviewed and merged into main . Note on documents in the odk-workflows folder IMPORTANT : Do not edit any files in the docs/odk-workflows/ directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker .","title":"Managing the documentation"},{"location":"omrse-doc-process/EditingOMRSEDocumentation/#updating-omrse-documentation","text":"NOTE: These instructions supersede the standard Managing Documentation workflow as specified in the ODK, except where explicitly mentioned below. You should follow only these instructions to create, edit, manage, curate, and maintain OMRSE documentation here on the GitHub repo. The documentation for OMRSE is managed in two places (relative to the repository root): The docs directory contains all the files that pertain to the content of the documentation (more below) The mkdocs.yaml file contains the documentation config, in particular its navigation bar and theme. The documentation is hosted using GitHub Pages, on a special branch of the repository (called gh-pages ). It is important that this branch is never deleted - it contains all the files GitHub Pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually . All changes to the docs happen inside the docs directory on the main branch.","title":"Updating OMRSE Documentation"},{"location":"omrse-doc-process/EditingOMRSEDocumentation/#editing-the-docs","text":"","title":"Editing the docs"},{"location":"omrse-doc-process/EditingOMRSEDocumentation/#changing-content","text":"All the documentation is contained in the docs directory, and is managed in Markdown . Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. There are two alternative workflows. The first workflow is preferred for more extensive editing including the creation of new documents and folders. The second workflow is more appropriate for minor edits to existing documents. All edits MUST follow one of the two workflows. Important: 1. ALL CHANGES MUST BE REVIEWED BEFORE BEING MERGED INTO THE main BRANCH! 2. Documentation of OMRSE classes, their definitions, other annotations, axioms, and so on should be done on Markdown pages in the modeling folder.","title":"Changing content"},{"location":"omrse-doc-process/EditingOMRSEDocumentation/#workflow-one","text":"Create a new branch either locally or on the GitHub site, depending on whether you will create and edit documents on the GitHub site (which is easier) vs. on your local machine. On the new branch, make your additions, changes, edits, etc. If you are working on the GitHub site, commit your changes to the new branch. If you are working locally, once your edits are completed, commit and push them (as well as the new branch if necessary) to the GitHub repo. Create a pull request from the new branch. Request a review of your changes.","title":"Workflow One"},{"location":"omrse-doc-process/EditingOMRSEDocumentation/#workflow-two","text":"Create a new .md file or open the existing .md file you want to change on the GitHub site. Once you have completed work on that file, click the green \"Commit changes...\" button. A dialog box will appear. You MUST click the \"Create a new branch for this commit and start a pull request\" radio button. Then write an informative commit message in the \"Extended description\" box and click \"Commit changes\". Create a pull request from the new branch. Request a review of your changes.","title":"Workflow Two"},{"location":"omrse-doc-process/EditingOMRSEDocumentation/#deploying-the-docs","text":"Follow the process described in the standard ODK instructions for Updating the Documentation . However, deployment of docs should occur only from the main branch, after the changes have been reviewed and merged into main .","title":"Deploying the docs"},{"location":"omrse-doc-process/EditingOMRSEDocumentation/#note-on-documents-in-the-odk-workflows-folder","text":"IMPORTANT : Do not edit any files in the docs/odk-workflows/ directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker .","title":"Note on documents in the odk-workflows folder"}]}